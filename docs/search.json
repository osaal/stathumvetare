[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Människovetaren och statistiken",
    "section": "",
    "text": "Förord\nFörst och främst: välkommen! Genom att öppna denna bok har du tagit dina första steg i att bli en statistiskt läskunnig forskare. Börja därför med att klappa dig själv på axeln.\nMänniskovetaren och statistiken är en öppen kursbok i statistisk analys för människovetare. Den meningen innehåller många nyckelord, så jag radar genom dem konsekvent.\nÖppen innebär att den är gratis - ja, helt free, nada dineros - och finns tillgänglig som nätbaserad bok. Du kanske läser den på nätet nu! Boken finns även tillgänglig för nedladdning som PDF-version (men notera dock att den inte är lika interaktiv), och förhoppningsvis även som fysisk kopia för er som tycker om doften av papper. Den sistnämnde kostar dock pengar, tyvärr.\nKursbok menar att boken i första hand är skriven för studiebruk. Bokens kapitel har som mål att hjälpa dig genom statistikens tidvis grumliga och mångtydiga värld. Detta menar dock inte, a), att du blir expert genom att läsa boken pärm-till-pärm, eller b), att du inte får läsa boken om du inte går kurs i statistik vid universitetet. Statistik lär du dig endast genom övning! Boken handleder dig genom inlärningsprocessen, men du måste själv lägga ned tid för att faktiskt lära dig något. För det andra är alla välkomna att lära sig statistik, oavsett studie- eller kursnivå. Däremot kan bokens innehåll vara något krävande för till exempel elever i högstadium. Låt dig inte skrämmas av innehållet, men var inte heller missnöjd med dig själv om du inte genast förstår allt.\nStatistisk syftar till förståelsen av världen genom siffror, modeller och samband. Detta begrepp kommer att öppna sig för dig genom boken. Ta dock redan med dig följande insikter:\nAnalys innebär att vi är här för att beskriva, tolka och förstå något. Det kan handla om människobeteende, kunskap, strukturer, tweets, videoessäer, ideologier, värderingar - ja, vad som helst. Det innebär även att den statistiska analysen inte enbart handlar om de tekniska tillämpningarna (medeltal, t-test och regressioner), utan även om förberedelse, datainsamling, tolkning, reproduktion, publikationer och mycket mer. Denna bok försöker handskas med helhetsprocessen av att utföra statistisk analys - i all dess rörighet, icke-linearitet och komplexitet.\nFör människovetare menar att boken är främst riktad åt studenter i de vetenskaper som försöker förstå människan i hennes helhet - beteende, miljö, produktion, socialitet, you name it. Det betyder förstås inte att en biolog inte får läsa boken, bara att den presenterar verktyg som biologen inte nödvändigtvis behöver lika mycket som psykologen eller sociologen. En asterisk bör dock noteras: ekonomer med ekonometrisk utbildning har möjligen mindre nytta av boken än andra. Detta är främst eftersom många av de verktyg som ekonomer använder sig av (t.ex. autoregression eller tidsserieanalys) inte finns beskrivna i denna bok.\nFör människovetare menar även att boken inte är riktad för studenter i sannolikhetsteori eller matematisk statistik. Vid vissa moment kommer jag att använda matematik, men jag är en sociolog, inte matematiker, och det syns förmodligen på dessa sidor. Det betyder även att du som läsare inte behöver kunna väldigt komplex matematik - kort matematik i gymnasiet (eller jämförbara kunskaper) torde räcka."
  },
  {
    "objectID": "index.html#bokens-struktur",
    "href": "index.html#bokens-struktur",
    "title": "Människovetaren och statistiken",
    "section": "Bokens struktur",
    "text": "Bokens struktur\nBoken är delad i fyra delar (fem, om du räknar denna del med).\nDel I presenterar det statistiska tankesättet. Målet med denna del är att få dig kalibrerad till att tänka kvantifierande: att omvandla världen till siffror för att säga något om dess struktur. Denna del är tämligen teoretisk, men hoppa inte över den! Jag anser att statistiskt tänkande är mångdubbelt viktigare för förståelsen av statistik än teknisk kunskap av modeller, och förhoppningsvis tänker du lika efter denna del av boken.\nDenna del handlar om statistik, definitionen av data, forskningsprocessen, variabler och modeller, samt statistisk inferens.\nDel II ger en introduktion till miljön där du (förmodligen) gör din statistisk analys: R. R är ett programmeringsspråk skapat av och för statistiska forskare. Här är det värt att säga några varningens ord:\n\nProgrammering tar en tid att lära sig. Du ska helst öva medan du läser, repetera exempel, och försöka utföra dina egna. Kom dock ihåg att det bara handlar om ett nytt “språk” - om du kan lära dig grunder i spanska, kan du lära dig grunder i R!\nDu behöver inte ha några som helst baskunskaper i programmering. Jag förmodar att läsaren är en novis i programmering. Om du dock kan programmering, så kommer du garanterat undan med en översiktlig läsning av denna del. Om du även kan R, så kan du hoppa över delen helt och hållet.\nAtt jobba med programmering innebär att testa, testa på nytt, och testa än en gång. Du kommer att göra otroligt många misstag, glömma parenteser, använda <- när du menade använda =… Detta är en naturlig del av att programmera, och inte en fördömning av dina kunskaper eller ditt egenvärde!\n\nDenna del gör dig bekant med R-miljön, installationen av R och RStudio, installationen och aktiveringen av paket, importering och exportering av data i olika format, samt grafisk uppställning.\nDel III innehåller vad många skulle kalla “köttet på de statistiska benen”2. Du blir bekant med flera olika statistiska modeller och metoder. Jag har försökt strukturera delen ungefär enligt svårighetsgrad. Vi inleder därmed med enkla metoder på enskilda variabler och ökar variabelmängden sakta men säkert. Jag har även valt att ta in två olika sorters innehåll: de främst använda metoderna inom människovetenskaplig statistisk forskning, samt nydanande metoder för studiet av komplexa och svårtillgängliga fenomen. Det finns dock fler statistiska metoder i världen än det finns forskare (inte nödvändigtvis sant, men riktgivande), och allt kan inte hanteras. Målet är därför att du blir statistiskt läskunnig, inte en vandrande encyklopedi av metoder och distributioner.2 Kanske enbart jag skulle kalla det så…\nVarje kapitel innehåller en beskrivning av metodens mål, krav och process. Därutöver ger jag exempelanalyser på verkliga data, för att synliggöra processen i att tolka siffrorna som R spottar ut. Kapitlen kan läsas isolerat, så att du kan återvända till de metoder som är relevanta för just dig. Jag rekommenderar faktiskt varmt, att du inte läser denna del i ett sträck. Återvänd när du behöver en viss metod, fördjupa dig i källhänvisningarna, och läs precis så mycket du behöver för att utföra uppgiften du har framför dig.\nDenna del beskriver uni-, bi- och multivariata analysmetoder, faktoranalys, nätverksanalys och simulationsanalys.\nDel IV återvänder till forskningsprocessen. Denna del beskriver hur en vetenskaplig publikation blir till, vad referentgranskning är, och hur du kan publicera data på nätet och jobba på ett källöppet sätt. Vetenskapen blir bättre av öppenhet, och det är guld värt av dig att lära dig jobba öppet, kollaborativt och dokumenterande från första början."
  },
  {
    "objectID": "vad-är-statistik.html#varför-siffror",
    "href": "vad-är-statistik.html#varför-siffror",
    "title": "1  Vad är statistik?",
    "section": "1.1 Varför siffror?",
    "text": "1.1 Varför siffror?\nAtt studera något statistiskt innebär att studera siffror. Den matematiskt mindre intresserade läsaren kanske ryggar tillbaka när hen hör detta: “Usch, jag var alltid så dålig på matematik i skolan, inte mera matte!”. Fear not, young padawan, för jag har ett viktigt budord:\n\nStatistik är inte matematik.\n\nVad jag menar är, att statistik inte handlar direkt om att räkna abstrakta talföljder, lösa ekvationer eller rita upp trigonometriska funktioner1. Statistik handlar i grund och botten om att svara på frågor.1 Fast statistik kan handla om det, om du vill. Den statistik du lärt dig i skolan är närmare detta än statistiken du lär dig i denna kurs.\nHurudana frågor besvarar vi? Här kommer budord två:\n\nStatistik besvarar frågan: “Finns det ett förhållande mellan X och Y?”\n\nDetta är allt statistik i grund och botten är. X och Y kan stå för vadsomhelst för fenomen: inkomsten mellan kvinnor och män, hemskolning i olika kommuner, kyrkobesökare under pandemiåren 2020-2022, och så vidare. Vad än vi kan kvantifiera kan vi ställa frågan om förhållande för. Och vi kan kvantifiera allting!\nHärifrån följer budord tre:\n\nBara för att du kan, innebär inte att du bör.\n\nAllting kan räknas och ställas om till siffror. Men statistik bör alltid börja med frågan om meningsfullhet: är det meningsfullt att du granskar förhållandet mellan X och Y? Detta är en teoretisk fråga, och en fråga som statistiken därmed inte kan besvara. Statistik är ett verktyg, bara du kan bestämma om verktyget passar din situation."
  },
  {
    "objectID": "vad-är-statistik.html#faror-och-fallgropar",
    "href": "vad-är-statistik.html#faror-och-fallgropar",
    "title": "1  Vad är statistik?",
    "section": "1.2 Faror och fallgropar",
    "text": "1.2 Faror och fallgropar\nJag har redan nämnt den första faran med statistik: avsaknaden av meningsfullhet. Då du gör statistiska analyser bör du alltid inleda från din frågeställning (mera om detta senare), inte från verktygen du använder. Om du vill veta något om verklighetens kvalitet kanske statistik inte är det rätta verktyget - kanske hermeneutiska metoder passar bättre. Men om du vill förstå verklighetens relationer mellan väldefinierade nivåer är statistik möjligen en mycket lämplig metod.\nMen vänta, vad då “väldefinierade nivåer”? Detta hänvisar till samma meningsfullhetsproblem som tidigare: Om du inte kan dra en teoretisk skiljelinje mellan två saker du vill analysera, kan du naturligtvis inte heller analysera dem i förhållande till varandra. Ett klassiskt exempel är rasforskning. Inom rasforskning intresserade man sig förrut om människorasernas skillnader i kognition och kunskap - intelligens, med andra ord. Eftersom denna forskningstematik handlar om skillnader, vet vi att statistik passar som verktyg.\nDet finns endast ett litet problem: “människoraser” finns inte. Nivåerna rasforskarna ville jämföra var aldrig väldefinierade. De ändrade utan rim eller resonemang, de baserades på människokvaliteter som inte var så distinkta som forskarna ville tro, för att nämna några problem. Om det teoretiska resonemanget faller sönder (vilket det gjorde inom rasforskningen), är det meningslöst att studera skillnader med statistik.\nMen märk: meningslöshet är inte samma sak som omöjlighet. Det görs rasforskning än idag; mycket av den moderna IQ-forskningen handlar ännu om detta. Du kan med andra ord kvantifiera det obefintliga. Kom ihåg budord tre: bara för att du kan, innebär inte att du bör!\nGenom kursen kommer du att bli bekant med olika sätt att analysera siffror och data. Du kommer förmodligen också att märka, att våra analyser inte bryr sig om vad vi matar in i dem. Du kan tänka dig statistiska analyser som kontextlösa fabriker: allt som kommer in (input) kommer att bli behandlat till en slutprodukt (output), oavsett vad som faktiskt kommer in. Textilfabrikens uppgift är att sy kläder, det har ingen skillnad för fabriken om råmaterialet är giftigt för människor. På samma sätt är statistikfabrikens uppgift att analysera siffror, det har ingen skillnad om siffrorna är meningslösa.\nPå grund av detta vill jag betona att statistik börjar med teori. Statistikens grundläggande verktyg är inte korrelationer, regressioner, korstabeller och variansanalyser - det grundläggande verktyget är frågeställningen. Utan en bra frågeställning kan du aldrig få bra svar, även hur avancerade dina analyser är. Kom ihåg detta när du börjar hopa dig in i de mest komplexa statistiska verktygens detaljer."
  },
  {
    "objectID": "vad-är-statistik.html#avslutningsvis",
    "href": "vad-är-statistik.html#avslutningsvis",
    "title": "1  Vad är statistik?",
    "section": "1.3 Avslutningsvis",
    "text": "1.3 Avslutningsvis\nDenna korta inledning betonar kanske de mer negativa aspekterna av statistik än de positiva. Jag vill dock från första början avfjärma dig från idén att statistik på något sätt är “mer vetenskapligt”, “mer sant” eller “mer äkta” än andra metoder eftersom den skulle handla om siffror och hårda fakta. Sanningen är, att statistiken bara är ett verktyg. Ett otroligt effektivt verktyg i en välformulerad undersökning, ett värdelöst verktyg i en dåligt formulerad undersökning.\nTill nästa ska vi ta itu med vad det är som studeras med statistik. Med andra ord: vad är “data”?"
  },
  {
    "objectID": "vad-är-data.html#datalivscykel",
    "href": "vad-är-data.html#datalivscykel",
    "title": "2  Vad är data?",
    "section": "2.1 Datalivscykel",
    "text": "2.1 Datalivscykel\nVi kan även förstå data genom att överväga dess livscykel metaforiskt. Data föds, data lever, och data dör.\nNär exakt data föds är - liksom med människor - lite av en filosofisk fråga. För enkelhetens skull kan vi säga att data föds när det har gjorts tillgängligt åt forskaren. Detta innebär att råfilen från ett enkätverktyg är data, men bibliotekets samling är inte data (förutom kanske för huvudbibliotekarien).\nEfter att data har fötts måste det hållas i liv för att kunna analyseras. Vi kan tala om datalivsstegen:\n\nRåa data.\nBehandlade data.\nAnalyserade data.\nPublicerade data.\n\nData går då från råvara till processerade varor till produkter och deras publikationer. Vi kan jämföra dessa livssteg med de nationalekonomiska industrierna: råvaruindustrin, förädlingsindustrin och serviceindustrin. Så som björk kan bli plankor och Ikea-möbler kan data bli variabler, analyser och publikationer. Enkätverktygens råfil är en dataråvara, medan den behandlade och summerade datafilen är en förädling, och de resulterande graferna och artikelmanuskripten är produkten.\nMånga forskare avslutar här, men moderna dataprocesser föreslår att livscykeln inte alls är över ännu! Efter att data har fullbordat sina ursprungliga syften bör de arkiveras, skyddas och katalogiseras. Du bränner inte upp en bok genast då du läst den klart - varför skulle du radera dina data?"
  },
  {
    "objectID": "vad-är-data.html#datakomplexitet",
    "href": "vad-är-data.html#datakomplexitet",
    "title": "2  Vad är data?",
    "section": "2.2 Datakomplexitet",
    "text": "2.2 Datakomplexitet\nSyftet med detta kapitel är att få dig att tänka på data som en mer komplex grej än bara några siffror eller intervjutranskriptioner. Data är verktyg med vilka vi kan berätta något om verkligheten, berätta historier, eller berätta våra åsikter. För att göra detta reliabelt måste våra data hållas rena, fina och dokumenterade. Våra analyser bör vara möjliga att upprepa, våra datamanipulationer genomskinliga och tydliga, våra data okorrumperade och tillgängliga. Detta kan förstås som dataintegritet, att våra data håller hög integritet genom hela livscykeln.\nDåligt dokumenterade, konstigt manipulerade, förfalskade, korrumperade eller obegripligt röriga data är inte analyserbara. Tänker du tillbaka till de tre definitionerna på data, så märker du att sådana data faktiskt inte är data! När du jobbar med dina data, håll därför i minnet några av de följande tipsen:\n\nDina data ska vara väldokumenterade. En utomstående ska kunna förstå vad alla siffror/tecken betyder, hur de har skapats och manipulerats, var de finns och hur de kan få tillgång till dessa data.\nSkriv loggböcker av dina analyser. Filosofen Bruno Latour rekommenderade fyra sådana: en för forskningsprojektets praktiska arrangemang, en för databehandlingen, en för skrivprocessens delmoment och en för projektets slutliga effekt på det som studerades. Detta kan vara lite overkill, men skriv gärna ned vilka datamanipulationer du gjort, vilka inställningar du använder, hur du tolkar och resonerar!\nDokumentation kan finnas så nära data som möjligt: vissa analysprogram tillåter att dokumentationen kopplas i datafilen, andra kräver separata dokument. Ju mera avstånd det är mellan dokumentationen och data, desto lättare är det att tappa kopplingen och göra datafilen obegriplig. Om du jobbar med R har du en fördel: alla manipulationer dokumenteras i dina script-filer!"
  },
  {
    "objectID": "vad-är-data.html#datadelning",
    "href": "vad-är-data.html#datadelning",
    "title": "2  Vad är data?",
    "section": "2.3 Datadelning",
    "text": "2.3 Datadelning\nSom tidigare sagt bör data skyddas, arkiveras och katalogiseras efter användningen. Ett sådant steg är en öppen datadelning. Detta innebär att du publicerar dina data offentligt i ett datalager (eng. data repository). Hur detta praktiskt görs behandlas i ett senare kapitel, men för att baka in detta som ett gott beteende ska vi inleda med att ladda ned data för senare analyser.\nEtt datalager är en service som någon upprätthåller, där du kan förvara dina datafiler för viss tid eller för evigt. Idén med ett datalager är lite som ett dokumentarkiv: i princip ska data förvaras för evigt, eller åtminstone så länge som det går med grundliga arrangemang. Det finns många olika datalager, vissa kostnadsbelagda och vissa gratis.\nZenodo är ett datalager som vi kommer att bruka senare. Gå till kapitel XXX om du vill veta mera om Zenodo.\nFör vårt bruk använder vi Finlands samhällsvetenskapliga dataarkiv, FSD eller även kallat Aila. Datalagret upprätthålls av Tammerfors universitet, och är gratis att bruka. I datalagret kan du finna både kvantitativa (siffror) och kvalitativa (texter) datafiler, och du kan även arkivera dina egna data med lite möd och arbete.\nPå Aila-hemsidan kan du välja att söka efter datafiler enligt namn, tillgänglighet, datatyp och språk. Datatillgänglighet är indelad i fyra kategorier:\n\nTillgänglig för alla utan registrering (licens CC-BY 4.0)\nTillgänglig med registrering för forskning, studier och undervisning\nTillgänglig med registrering endast för forskning, inklusive högre lärdomsprov och doktorsavhandlingar\nTillgänglig endast med tillstånd\n\nRegistrering innebär inloggning med Haka-federationens lösen, vilka är tillgängliga åt dig automatiskt om du är student vid en finländsk högskola. Om du har problem med Haka-inloggning, följ instruktionerna din högskola ger dig (t.ex. på intranätet eller genom att kontakta IT-stödet vid din högskola). Om du inte har Haka-lösen kan du endast använda data i kategori A - men det finns mycket av dessa, och A-data är inte sämre än andra data!\nTillgängligheten ställs in av forskaren enligt principen så öppet som tillåtet, så stängt som krävt. Detta innebär att datafiler i princip ska öppnas för allmänheten (A), men begränsningar så som personsekretess, licenser och dylikt kan medföra mer strikta användningsnivåer (B-D)."
  },
  {
    "objectID": "vad-är-data.html#övning-ladda-ned-finlandssvenska-barometern-2008",
    "href": "vad-är-data.html#övning-ladda-ned-finlandssvenska-barometern-2008",
    "title": "2  Vad är data?",
    "section": "2.4 Övning: Ladda ned Finlandssvenska barometern 2008",
    "text": "2.4 Övning: Ladda ned Finlandssvenska barometern 2008\nI denna kursbok använder vi oss av en specifik datafil: Finlandssvenska barometern 2008. Du kan prova hitta den genom Aila själv, eller klicka på denna länk.\nFölj stegen till att ladda ned datafilen, i formen av en Zip-fil. Detta är en komprimerad mapp, vilket innebär att den måste avkomprimeras med ditt favorit zip-program1.1 Något alla vanliga människor har, förmodar jag.\nOm du lyckas avkomprimera filen hittar du en mapp med några textfiler och ytterligare en mapp, Study. Textfilen README.txt innehåller information om de data du nyss laddade ned, inklusive en listning av alla filer som borde finnas med samt en licensbeskrivning.\nGenom att ladda ned filen har vi godkänt att vi licenserar filen med licensen CC-BY 4.0. Detta står för “Creative Commons Attribution 4.0 license”, och är en slags öppen licens för diverse resurser. Det viktiga som du måste beakta är licensdelen “-BY”, alltså “Attribution”. Detta betyder att du måste (inte en rekommendation, ett krav!) säga vem som har skapat datafilen när du använder den. Du får manipulera den, du får ompublicera den, du får förtjäna pengar på den, men du måste säga vem som skapade den ursprungligen.\nFinlandssvenska barometern 2008 är skapad av Kjell Herberts vid Åbo Akademi - som det står i README.txt.\nI mappen Study hittar du två PDF-filer och en ytterligare mapp, data. PDF-filen vars namn börjar med cb är kodboken (eng. code book, därför cb), och är ytterst viktig! I kodboken hittar du en förklaring på hur dessa data är insamlade, vilka variabler som finns i datafilen, vad alla siffervärden står för, samt den ursprungliga enkäten. PDF-filen som börjar med qu är den ursprungliga enkäten (står för eng. questionnaire, därför qu), alltså samma som du finner i slutet av kodboken.\nOm du kan finska, bekanta dig gärna med kodbokens innehåll i lugn och ro. Om inte, så kommer jag under bokens lopp att översätta relevanta delar - ingen fara!\nInom undermappen data hittar du det viktigaste: datafilerna! Det finns tre filer: en med ändelsen .csv, en med ändelsen .por, och en med ändelsen .html.\n.csv-filen är vad vi kommer att använda. Förkortningen står för comma-separated value, filen är ett slags öppet format för hanteringen av större mängder data i tabularformat. Detta innebär att varje rad är en observation eller en respondent, varje kolumn är en variabel och varje cell (ruta) är ett datavärde. Vi hanterar senare vad dessa betyder. Du kan öppna filen i ett kalkylbladsprogram (Microsoft Excel, LibreOffice Calc, Google Sheets eller liknande) om du vill se innehållet. Då möts du av något som detta:\n[Bild av datafilen]\nObegripligt, eller hur? Men ingen fara, vi kommer att sakta bearbeta oss genom filens innehåll och förstå vad allt betyder."
  },
  {
    "objectID": "vad-är-data.html#avslutningsvis",
    "href": "vad-är-data.html#avslutningsvis",
    "title": "2  Vad är data?",
    "section": "2.5 Avslutningsvis",
    "text": "2.5 Avslutningsvis\nGrattis, du har påbörjat din resa i forskningens värld genom att bekanta dig med konceptet av data och dess innebörd! Det finns dock några ytterligare teoretiska koncept vi måste öva före vi kan hoppa in i den praktiska analysen. Först tar vi ett fågelperspektiv: hur går forskningsprocessen till väga? Hur kommer du genom datalivscykeln, vad händer på vägen, och är du samma människa när du hoppar ut som när du hoppade in? Svaret på den sista frågan beror på din livsfilosofi, men de övriga handlar om forskningsprocessen."
  },
  {
    "objectID": "från-ide-till-rapport.html#forskningsobjektet-och-frågeställningen",
    "href": "från-ide-till-rapport.html#forskningsobjektet-och-frågeställningen",
    "title": "3  Från idé till rapport",
    "section": "3.1 Forskningsobjektet och frågeställningen",
    "text": "3.1 Forskningsobjektet och frågeställningen\nFör att forska behöver vi ett forskningsobjekt. Vad vill vi studera? Är det en person, en organisation, en samhällsstruktur, eller något annat? Forskningsobjektet kan vara hur abstrakt eller konkret som helst, det kan vara frågan om en ideologi eller en hjärna.\nHur väljer man ett forskningsobjekt? Detta kommer från dina egna intressen! Vad eller vem vill du studera? Du kan börja med att fundera ut vad som gör forskningsobjektet intressant - varför vill du studera det? Frågan om intresse leder snabbt in till en rad olika frågeställningar. Ett exempel kan hjälpa.\nJag är intresserad av datorspelet Sid Meyer’s Civilization VI. Spelet är ett strategispel där du bygger ett historiskt rike, utvecklas i kultur och vetenskap, för krig, sprider religion, upprätthåller diplomatiska relationer, och så vidare. Jag har förmodligen lagt ned fler timmar i att både spela och läsa om Civilization VI än jag har lagt ned på denna bok. En av de största dragen till spelet för mig är hur versatilt och varierande det är: det finns flera olika sätt att vinna ett spel, det finns nästan oändligt många sätt spelet beter sig över tid, och därför kan man spela det på nytt flera gånger.\nHär finns redan flera frågeställningar. Som sociolog är jag intresserad av människor, deras kunskapssystem och sociala system. Därför kan jag tänka mig att t.ex. fråga: - Vilka sociala grupper spelar Civilization VI? - Finns det skillnader i vem (vilka sociala grupper) som tenderar att vinna spelet genom krig? - Spelar människor oftare med religioner de själva följer? Vore jag en psykolog kanske jag skulle intressera mig för frågor så som: - Är spelets beroendeframkallande natur kopplat till de snabba spelaktionerna? - Hur ändrar graden av spelvinster över individens utveckling som barn och ung vuxen? - Har spelarna personlighetstyper som skiljer sig från den allmänna populationen? Om jag istället forskade inom medier kunde frågorna lyda: - Vilka paramedier (wiki-sidor, YouTube-kanaler, TikTok-producenter…) konsumerar spelarna? - Hur vanligt är produktion av media angående spelet? - Hur ofta talar datorspelmedia om Civilization VI jämfört med andra strategispel?\nMärk, att alla dessa frågor kan uttryckas som förhållanden mellan två eller flera saker: “Vilka sociala grupper spelar Civlization VI?” är det samma som “Graden av spelande mellan olika sociala grupper”, alltså ett förhållande mellan “spelande” och “social grupp”. “Är spelets beroendeframkallande natur kopplat till de snabba spelaktionerna?” är det samma som förhållandet mellan “beroendeframkallande natur” och “snabba spelaktioner”. “Hur vanligt är produktion av media angående spelet?” är det samma som förhållandet mellan “alla spelare” och “medieproducenter”. Du ser säkert logiken. Genom att omformulera frågeställningen till en fråga om förhållanden (t.ex. skillnader, likheter, grader) kan vi skapa en statistisk frågeställning.\nValet av frågeställning kan inte lösas med statistik. Du måste se på frågan, fundera ut vad det skulle innebära att besvara den, och huruvida du kan och vill lägga ned tiden för att besvara frågan."
  },
  {
    "objectID": "variabler.html#vad-är-en-variabel",
    "href": "variabler.html#vad-är-en-variabel",
    "title": "4  Variabler",
    "section": "4.1 Vad är en variabel?",
    "text": "4.1 Vad är en variabel?\nLåt oss säga att du vill undersöka följande frågeställning: “Är hunger relaterat till ökade matköp?” Du har bestämt dig att du undersöker frågan genom ett experiment, där du skickar 50 personer till Lidl för matköp. Du har valt att ställa upp ett kontrollerat experiment, så 25 av personerna är hungriga och 25 är mätta. Data du samlar är i formen “Euro spenderat på mat”, samt några bakgrundsfrågor från en enkät: kön och inkomst.\nVariabler är små lådor som kan innehålla någon form av data. En variabel innehåller data av en typ. Variabeln innehåller olika data för varje analysobjekt, eller med andra ord: variabeln varierar mellan analysobjekten.\nI exemplet ovan finns därför följande variabler:\n\nGrupp (hungrig eller mätt)\nMatköp (euro spenderat på mat)\nKön (man, kvinna eller annat)\nInkomst (bruttoeuro i året)\n\nVi kan tänka oss en slumpmässig person i experimentet. Hen tar olika värden på alla variabler, men enbart ett värde per variabel: - Grupp = “mätt” - Matköp = 75 - Kön = “kvinna” - Inkomst = 40000\nMed andra ord: vår person var en mätt kvinna med en bruttoårsinkomst på 40 000 euro, som köpte mat för 75 euro under experimentet.\nTanken bakom variabler är att vi kan jämföra olika analysobjekt på samma skalor: vilka kön är representerade i vårt datamaterial? Vad är de största och minsta matköpen som våra analysobjekt gjorde? Vi kan även försöka besvara vår frågeställning genom en uppsättning noll- och alternativa hypoteser:\n\n\\(H_{1a}\\): Hungriga personer köper mera mat än mätta personer.\n\n\n\\(H_{1b}\\): Hungriga personer köper mindre mat än mätta personer.\n\n\n\\(H_{0}\\): Det finns ingen skillnad i matköp mellan hungriga och mätta personer."
  },
  {
    "objectID": "variabler.html#variabelförhållande-1-beroende-och-oberoende-variabler",
    "href": "variabler.html#variabelförhållande-1-beroende-och-oberoende-variabler",
    "title": "4  Variabler",
    "section": "4.2 Variabelförhållande 1: Beroende och oberoende variabler",
    "text": "4.2 Variabelförhållande 1: Beroende och oberoende variabler\nOm vi funderar på vår frågeställning, så finns det en inbyggd kausal1 riktning. I vår frågeställning utreder vi egentligen huruvida hunger orsakar ökade matköp. Hypoteserna ställer upp påståenden i förhållande till denna kausalitet: \\(H_0\\) handlar om avsaknaden av kausalt förhållande, medan \\(H_{1a}\\) och \\(H_{1b}\\) bägge handlar om på vilket sätt hunger orsakar ökade matköp.1 Kausalitet är, förenklat, idén att fenomen har orsak och verkan. A är orsaken till B, och B är därmed A:s verkan.\nFöre vi går vidare, ett varningens ord: definitionen av en variabel som beroende definierar inte att det faktiskt skulle finnas kausalitet, även om du hittar ett förhållande i dina data. Detta kan vara svårbegripligt, men du har säkert hört detta påstående i en annan form: korrelation är inte kausalitet. [MERA HÄR]\nVi kan klassificera variabler i beroende och oberoende variabler. Tänker vi tillbaka på kausaliteten i vår frågeställning, så kan vi med lite logik översätta våra termer:\n\nHunger orsakar ökade matköp.\nÖkade matköp är orsakade av hunger.\nÖkade matköp är beroende av hunger.\n\nSåledes är “matköp” vår beroende variabel! De variabler som blir utanför, men som vi använder i vår analys, är våra oberoende variabler (eftersom de inte, i denna analys, beror på någonting utan enbart finns till).\nDe flesta statistiska analyser utförs på en beroende variabel och en eller flera oberoende variabler. Fler än en beroende variabel kan i vissa fall användas (och vi blir bekanta med en sådan situation, vid multipel variansanalys eller MANOVA), och enbart en oberoende variabel är också möjligt."
  },
  {
    "objectID": "variabler.html#variabelskalenivåer",
    "href": "variabler.html#variabelskalenivåer",
    "title": "4  Variabler",
    "section": "4.3 Variabelskalenivåer",
    "text": "4.3 Variabelskalenivåer\nFör att kunna besvara våra hypoteser måste vi utföra något slags statistiskt test. Enligt NHST-principen ska vi specifikt utföra ett statistiskt test som kan bevisa nollhypotesen fel, alltså ett test som kan visa att det faktiskt finns skillnad i matköp mellan hungriga och mätta personer.\nAv diverse matematiska orsaker finns det olika statistiska test för olika former av data. En stor del av denna bok handlar om dessa test - men det finns nästan oändligt många. Dataformen som är relevant är variablernas skalenivåer. (eng. scale level). Följande figur exemplifierar dessa:\n\n\n\n\n\nVariabelskalenivåer\n\n\n\n\nSkalenivåerna representerar variabeln på ett spektrum från kvalitativa till kvantitativa variabler. Terminologin kan vara lite förbryllande. Vi kan tänka oss att en variabel kan ha fyra olika fundamentala egenskaper:\n\nAbsolut nollpunkt innebär att det finns en logisk nollpunkt på skalan, som har en betydelse. Exempelvis inkomst (0 € skiljer mellan de som förtjänar pengar och de som förlorar pengar) och temperatur (0\\(\\circ\\) är vattnets fryspunkt vid en atmosfär med tryck) har en nollpunkt. Kommuner, däremot, har inte: det finns inget som “noll kommun”.\nIntervall innebär att avstånden mellan punkterna på skalan är lika långa. Inkomst är igen ett exempel på detta: skillnaden mellan 5 och 6 euro är lika stor som skillnaden mellan 50 000 och 50 001 euro (i bägge fallen, en euro).\nOrdning innebär att variabelns värden kan ordnas från minst till störst. Detta gäller förstås för inkomst.\nSkillnad innebär att variabelns värden skiljer mellan olika kategorier. Vi kan förstå inkomst som en nästan oändlig rad av kategorier, separerade enligt varje euro (eller cent).\n\nOm vi ställer upp dessa i en tabell kan vi se hur variablerna skiljer sig från varandra:\n\n\n\n\nAbsolut nollpunkt\nIntervall\nOrdning\nSkillnad\n\n\nKvot\nX\nX\nX\nX\n\n\nIntervall\n\nX\nX\nX\n\n\nOrdinal\n\n\nX\nX\n\n\nNominal\n\n\n\nX\n\n\n\nKvotvariabler är de “mest” kvantitativa variablerna (vad det än betyder…). Kvotvariabler skiljer mellan variabelvärden, kan ställas i ordning från minst till störst, har lika stora mellanrum mellan varje värde, och har en meningsfull nollpunkt. Med andra ord uppvisar de alla fyra egenskaper. Riktiga kvotvariabler är något sällsynta i människoforskning.\nIntervallvariabler är ett steg ifrån kvotvariabler. Intervallvariabler skiljer mellan värden, kan ställas i ordning, och har lika stora mellanrum, men de saknar en meningsfull nollpunkt. Liksom kvotvariabler är dessa något sällsynta i människoforskning, men de både finns, och i vissa situationer kan vi konstruera sådana genom index.\nOrdinalvariabler är ett steg vidare. Ordinalvariabler skiljer mellan värden och kan ställas i ordning, men stegen mellan värden är inte längre likstora och det finns inte heller en nollpunkt. Denna variabeltyp är mycket vanlig inom människovetenskaper, bland annat i formen av Likert-skalor.\nNominalvariabler är de “mest” kvalitativa variablerna. De uppvisar endast skillnad, men ingen meningsfull ordning, inga jämna steg mellan värden och ingen nollpunkt. Dessa är även typiska i samhällsforskning.\nVissa forskare brukar kalla kvot- och intervallvariablerna för kvantitativa, och ordinal- och nominalvariablerna för kvalitativa, medan andra inkluderar ordinalvariablerna inom de kvantitativa. Jag anser att detta påvisar att skiljelinjen är något flytande, istället för en hård linje, och att det är mer meningsfullt att överväga vilka metoder som kan tillämpas på de olika skalenivåerna.\nVi kan även tala om multikategoriska variabler. Dessa är helt enkelt variabler som har fler än två kategorier - oavsett om de är nominal-, ordinal-, intervall- eller kvotvariabler. Ofta brukar man inte kalla intervall- och kvotvariablernas värden för “kategorier” (eftersom det börjar bli meningslöst med, t.ex. femtiotusen kategorier), men tekniskt sett har alla variabler kategorier - vissa är bara så små att vi inte anser dem vara kategorier.\nBinära variabler bör nämnas skiljt. Dessa har enbart två kategorier, och är ofta nominalvariabler. Däremot går binära variabler att använda i analyser som vanligen kräver intervall- eller kvotvariabler. Detta beskrivs i större noggrannhet i kapitlet 8.1.1 [FIXA REFERENS]\nOm vi granskar variablerna i vårt exempel kan vi relativt lätt bestämma vilka skalenivåer de uttrycker. “Hunger” är mätt som en binär variabel: analysobjektet är antingen hungrigt eller mätt. Det har egentligen ingen betydelse vilka siffror vi använder för att representera detta, men av konvention brukar man använda 0 för avsaknad av effekt (“Inte hungrig”, alltså “mätt”) och 1 för förekomst av effekt (“Hungrig”). Matköp och inkomst är bägge kvotvariabler, eftersom de har absoluta nollpunkter (0 euro spenderat på mat respektive 0 euro bruttoinkomst i året). Kön är en multikategorisk nominalvariabel, eftersom vi har mätt kön i tre kategorier och det inte finns någon ordning mellan kategorierna.\nOm du följer med noga har du kanske märkt att jag ofta säger “mätt som” och “vi har mätt” i diskussionen. Detta är för att ingen operationalisering automatiskt har en viss skalenivå. Vi kan alltid välja att mäta kön genom graden av människor som skulle kalla personen ett visst kön (kvotvariabel), matköp genom mycket eller lite matköp (binär), inkomst enligt inkomstpercentiler (multikategorisk ordinalvariabel), och så vidare. Kvot- och intervallnivåer är behändiga, eftersom vi kan utföra väldigt komplexa statistiska analyser på dem, men allt är inte rimligt att mäta så - mitt exempel på att mäta kön på kvotnivå är ett exempel på en orimlig analys.\nVi kan även omvandla mellan skalenivåer, men enbart åt ett håll: vi kan “förenkla” variabler ner från kvantitativa ändan mot kvalitativa ändan, men inte direkt vice versa. Inkomst kan därmed kategoriseras i hög, medel och låg inkomst, vilket skapar en multikategorisk ordinalvariabel, men kön kan inte kvantifieras till en skala av “könhet”."
  },
  {
    "objectID": "variabler.html#variabelförhållande-2-medierande-dämpande-och-externa-variabler",
    "href": "variabler.html#variabelförhållande-2-medierande-dämpande-och-externa-variabler",
    "title": "4  Variabler",
    "section": "4.4 Variabelförhållande 2: Medierande, dämpande och externa variabler",
    "text": "4.4 Variabelförhållande 2: Medierande, dämpande och externa variabler\nNär vi utökar analysen från bivariat (två variabler) till multivariat (flera variabler), kan vi börja tala om mer komplexa förhållanden än bara beroende och oberoende variabler. I en multivariat analys inför man ofta kontrollvariabler. Deras uppgift är att kontrollera för någon variation som vi inte vill att ska påverka resultatet.\nOrsaken varför man gör multivariata analyser är att man vill elaborera ett samband: Vad kunde sambandet bero på? Finns det andra förklaringar till sambandet, eller är vår förklaring den bästa? Finns det situationer där sambandet försvinner eller blir starkare?\nFör att exemplifiera detta, låt oss tänka oss några ytterligare hypoteser på vårt hungerexempel:\n\n\\(H_2\\): Män påverkas av hunger i större grad än kvinnor och icke-binära vad kommer matköp.\n\\(H_0\\): Kön har ingen påverkan på förhållandet mellan hunger och matköp.\n\nNu har vi lagt till en kontrollvariabel, kön, i vår analys. Vi kan testa om kön modifierar förhållandet mellan hunger och matköp. Hur gör vi detta? Jo, analytiskt sett fördelar vi vår analys upp i mindre delar:\n\nMatköp när hunger = 1 vs. matköp när hunger = 0 (detta är vårt ursprungliga test)\nMatköp när hunger = 1 och kön = 1 vs. matköp när hunger = 0 och kön = 1 (effekten hos män)\nMatköp när hunger = 1 och kön = 2 vs. matköp när hunger = 0 och kön = 2 (effekten hos kvinnor)\nMatköp när hunger = 1 och kön = 3 vs. matköp när hunger = 0 och kön = 3 (effekten hos icke-binära)\n\nSedan kan vi jämföra de olika sambandsmåtten vi har fått mellan testen 2-4. För att testa vår nya nollhypotes, att kön inte har en påverkan, granskar vi huruvida sambandsmåtten för testerna är desamma, alltså att T2 = T3 = T4.\nNär man kontrollerar för en variabel testar man ifall kontrollvariabeln kunde förklara det ursprungliga sambandet. Säg att vi finner ett samband mellan hunger och matköp, alltså vi ser att hungriga personer köper mer mat än mätta personer. Om vi inför kön som kontrollvariabel, och finner att sambandet försvinner, så kan vi säga att det faktiskt var kön som orsakade sambandet. Orsaken varför vi trodde att det var hunger kan vara att ett kön oftare var hungrigt än ett annat - så vi kunde inte skilja mellan hunger och kön i vår analys före vi kontrollerade för kön. I detta fall säger vi att vårt samband var spuriöst. Till synes fanns det ett samband, men när vi kontrollerade för externa variabler så försvann sambandet.\nMan kan även finna att ett samband blir starkare eller svagare när man kontrollerar för en variabel. Då säger vi att kontrollvariabeln är en amplifierande (förstärker sambandet) eller dämpande (försvagar sambandet) variabel.\nTill sist kan vi även finna att ett samband förekommer först efter att vi kontrollerat för en extern variabel. I det fallet kallar vi kontrollvariabeln för en medierande variabel eller mediatorvariabel, eftersom den medför ett samband.\nAlla dessa termer kan vara svåra att minnas, och desto svårare att skilja mellan i praktisk analys, så det är värt att repetera dem ofta och väl. Elaboreringsprocessen är en viktig del av forskningen, eftersom vi vill vara säkra om att vi förstår ett potentiellt samband väl. Om vi påstår att ett samband finns, men någon annan finner att det faktiskt var ett spuriöst samband och att en kontrollvariabel förklarar våra resultat, så är det en ganska genant situation…\n[SKAPA GRAFIK FÖR VARIABELFÖRHÅLLANDEN]"
  },
  {
    "objectID": "statistisk-inferens.html#inledning",
    "href": "statistisk-inferens.html#inledning",
    "title": "6  Statistisk inferens",
    "section": "6.1 Inledning",
    "text": "6.1 Inledning\nNär vi forskar om något vill vi berätta något om det - förstås. Vi vill beskriva hur världen är, hur den fungerar, hur människan beter sig, organisationens delar, und so weiter. Vi kan göra denna beskrivning på många olika sätt, men i denna bok fäster vi oss vid den statistiska beskrivningen.\nVerkligheten är dock stor - otroligt stor. Det finns många människor (snart åtta miljarder vid dagsdato!), många åsikter, många mönster, många sätt att vara.\nTänk dig följande exempel: Du vill mäta hur många röster de olika politiska partierna skulle få om ett riksdagsval utfördes nu. Valröstning är ganska enkelt att kvantifiera: en person ger en röst åt ett parti från en förbestämd lista. För att mäta graden av röster för partierna kunde du alltså helt enkelt fråga den finländska röstberättigande befolkningen vem de skulle rösta på just nu.\nMen tänk lite noggrannare, så märker du säkert att det finns flera problem här. Hur ska du fråga alla fyra-eller-så miljoner röstberättigade finländare? Vem betalar för detta projekt? Vad ska du göra om ett politiskt parti upphör att existera under din datainsamling?\nEftersom det är praktiskt taget omöjligt att göra sådana totalundersökningar som den ovan beskrivna partiundersökningen, har statistiker och människovetare utvecklat många verktyg för att härleda resultat från mindre grupper. Dessa grupper kallas urval eller sampel, och deras definition, begränsningar, metoder för undersökning, och härledda statistiker är både varierande och mycket viktiga att förstå.\nI detta kapitel kommer du att lära dig om denna process av härledning, vilket kan sammanfattningsvis kallas statistisk inferens. Du lär dig om olika sorters urvalsramar, förhållandet mellan urval och population, systematiska och osystematiska felmätningar, statistiska fördelningar, samt olika former av inferens- eller osäkerhetsmått. Detta kapitel hanterar de teoretiska grunderna - senare i boken kommer du att lära dig hur osäkerhet mäts i praktisk forskning."
  },
  {
    "objectID": "statistisk-inferens.html#den-kvantitativa-ryggraden",
    "href": "statistisk-inferens.html#den-kvantitativa-ryggraden",
    "title": "6  Statistisk inferens",
    "section": "6.2 Den kvantitativa ryggraden",
    "text": "6.2 Den kvantitativa ryggraden\nDen statistiska härledningsprocessen kan delas upp i fem grundläggande steg:\n\nParametrar\nEstimering\nStandardfel\nIntervall av konfidens\nNollhypotessignifikanstestning\n\nVarje steg berättar något mera åt oss om det vi har mätt, och därigenom om verkligheten.\nVi inleder med det vi vill veta, parametern. Det kan handla om ett medeltal, ett förhållande mellan två variabler, graden av förändring av en variabel över tid, eller något annat. Med andra ord: vad vill vi veta om verkligheten? Matematiskt kan vi förstå denna parameter som en okänd variabel:\n\\[X\\]\nEftersom vi har tagit ett urval, så kan vi strikt taget inte säga något om den verkliga parametern. För att veta något om t.ex. könsfördelningen i Finland måste vi ju veta den exakta mängden invånare inom varje könskategori, och det kräver en totalundersökning. Men allt är inte förlorat: vi kan estimera parametern. En estimering, eller uppskattning, är väldigt enkel: den utgörs av parametern, tillsammans med någon grad av felmätning. Med andra ord: “Jag uppskattar könsfördelningen vara på detta sätt, plus/minus lite felmätning”. Matematiskt kan vi skriva det som en enkel formel:\n\\[X = \\bar{x} + \\delta\\]\ndär \\(\\bar{x}\\) (uttalas ‘x-bar’) är vårt estimat och \\(\\delta\\) (grekiska delta, liten bokstav) är vår felmätning. I detta skede behöver vi inte bry oss om hur stora dessa siffror är, eller vad de egentligen står för.\nFör att förstå hur mycket felmätning det finns i vårt estimat kan vi ta till ett statistiskt verktyg: standardfelet (eng. standard error). Standardfelets beräkning görs genom:\n\\[ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\]\neller i klarspråk: estimatets standardfel är lika med dess standardavvikelse (eng. standard deviation) dividerat med kvadratroten av urvalsstorleken. Nu behöver du inte tänka alltför länge på denna beräkning - du blir bekant med den senare i kapitel XXX. Vad som är viktigt att förstå är att vi kan räkna en exakt siffra på hur mycket felmätning vi tror att vi har i vårt estimat. Denna felmätning beräknas genom spridningen av datapunkterna runtom vårt estimat - något du lär dig mera om senare.\nI detta skede har vi redan åstadkommit mycket: vi vet att det finns ett värde som vi mätt (parametern), vi vet att vi har mätt det med onoggranna metoder (estimatet), och vi kan beräkna hur mycket onoggrannhet vi har i vårt mått (standardfelet). Detta säger dock inte ännu huruvida vårt estimat är korrekt, eller hur korrekt det är. För det behöver vi några flera mått: konfidensintervallet och nollhypotessignifikanstestningen.\nKonfidensintervallet mäter hur mycket felmätning vi skulle åstadkomma i det långa loppet om vi upprepade vårt urval och estimat oändligt många gånger. Hur man beräknar konfidensintervallet kommer vi tillbaka till. Kort sagt beskriver det hur säkra vi är i urvalets påverkan på vårt estimat - hur mycket av vårt estimat är beroende av vårt urval, och hur mycket är faktiskt korrekt? Vi vill ju säga något om verkligheten i alla situationer, inte bara i vårt urval, så vi måste veta om vi prickar rätt med våra metoder.\nNollhypotessignifikanstestningen (NHST) är en statistisk filosofi som lägger grunden för all statistik vi behandlar i denna bok1. I skillnad till konfidensintervallet, där vi mätte graden av felmätning i det långa loppet, så mäter NHST graden av felmätning i detta urval. Det är alltså ett sätt att kvantifiera hur mycket osäkerhet vi finner i vårt urval, i förhållandet till hur mycket osäkerhet vi vill tolerera. NHST är en filosofi med ungefär 100 år av misstolkningar, grubbel och manipulation, och därför handlar en stor del av detta kapitel om vad NHST kan (och inte kan) göra.1 Boken handlar om frekvensstatistik. Det finns alternativa synsätt på statistisk forskning, så som Bayesisk statistik, men dessa behandlas inte i boken.\n\n6.2.1 Sammanfattning\n\nDen statistiska processen innehåller fem steg: parametrar, estimering, standardfel, konfidensintervall och nollhypotessignifikanstestning.\nParametern är det mått vi vill räkna ut, t.ex. könsfördelningen i Finland.\nEstimering innebär att vi uppskattar parametern med någon grad av felmätning.\nStandardfelet är en kvantifiering av felmätningen i vårt estimat.\nKonfidensintervallet berättar hur mycket felmätning vår metod orsakar i det långa loppet.\nNollhypotessignifikanstestningen berättar hur mycket felmätning vi orsakade denna gång."
  },
  {
    "objectID": "statistisk-inferens.html#statistiska-fördelningar",
    "href": "statistisk-inferens.html#statistiska-fördelningar",
    "title": "6  Statistisk inferens",
    "section": "6.3 Statistiska fördelningar",
    "text": "6.3 Statistiska fördelningar\nHur kan vi säga något om felmätningsgraden i vårt parameterestimat, då vi inte vet något om den verkliga parametern? Hur kan vi veta hur noga vi mätte könsfördelningen i Finland, om vi inte vet hur den faktiskt ser ut? Detta dilemma kan se ganska olösligt ut! Som tur är, har några mycket klipska statistiker och matematiker utvecklat verktyg med vilka vi kan estimera det okända: fördelningar.\nEn fördelning är en föruppfattning om sannolikheten för vissa värden. Fördelningar berättar alltså för oss hur sannolika vissa värden är. Nyckeln här är dock “föruppfattning”: för att vi ska kunna säga något om värdets sannolikhet, måste vi först bestämma för oss att denna sannolikhet har en fördelning som vi kan definiera. Detta kan låta påhittat, som att vi bara drar sannolikheter ur hatten med ingen respekt för verkligheten, men i själva verket fungerar vi som människor väldigt ofta på samma sätt! När jag går på gatan och min generaliserade ångeststörning frågar mig, “Hej du! Vad om en asteroid just nu skulle falla och krossa dig? Vad då?”, så kan jag ju faktiskt inte säga huruvida det kommer eller inte kommer att hända. Det är i framtiden, det okända, och jag måste härleda mig till ett svar. Jag kanske svarar “Ja du, det är väl ganska osannolikt”, eller så svarar jag “Skit då, det kommer säkert att hända, jag kommer att dö snart!”2.2 Eller, om jag följer terapeutiska metoder, så svarar jag förhoppningsvis “Det kan hända, eller så händer det inte, det är bara att vänta och se.”\nI bägge av svaren ovan, så ger jag egentligen asteroidens nedslag en sannolikhet. Högst troligen inte i siffror, men en sannolikhet likväl. Fördelningar gör samma sak, men de förmodar en kvantitativ sannolikhet - de lägger siffror på denna osäkerhet!\nVarför är fördelningar viktiga? Jo, för att om vi kan med tillräcklig säkerhet säga att vår parameter kommer från en viss fördelning, så kan vi beräkna hur sannolikt det är att få ett estimatvärde eller ett annat! Om vi tänker tillbaka till asteroidexemplet, så kan vi preliminärt ge några siffror. Enligt Wikipedia-artikeln “Impact event” slår en asteroid med en diameter på högst 1 kilometer ner på Jorden ungefär en gång per femhundratusen år. För enkelhetens skull förmodar vi att jag skulle dö i 100% av fallen där detta sker (oavsett hur nära jag är nedslaget). Sannolikheten för att jag dör i år av att bli krossad av en asteroid är därmed:\n\\[p(\\text{dö}) =  \\frac{1}{500000} = 0.000002\\]\nSannolikheten att jag inte dör av ett asteroidnedslag är inversen av detta:\n\\[p(\\text{inte dö}) = 1-p(\\text{dö}) = 1-0.000002 = 0.999998 \\]\nSo far so good, chansen tycks vara väldigt liten för att dö, och väldigt stor för att inte dö. Dessa två siffror kan också ställas upp i en graf, där x-axeln beskriver de olika alternativen (dö, inte dö), och y-axeln beskriver deras unika sannolikhet. Figur @ref(fig:asteroid-pdf) åskådliggör detta.\n\nbarplot(names.arg = c(\"Inte dö\", \"Dö\"),\n        height = dbinom(0:1, size=1, p=0.000005),\n        xlab = \"Utsaga\", ylab=\"Sannolikhet\",\n        ylim = c(0,1)\n        )\n\n\n\n\nSannolikheten för att jag dör i asteroidnedslag\n\n\n\n\nVad jag har genererat här är faktiskt en sannolikhetsfördelning Den kallas även en bernoullifördelning, nämnd efter matematikern Jacob Bernoulli (1655/1654-1705). Bernoullifördelningen berättar åt oss sannolikheten för en binär variabel, alltså då utsagan kan vara en av två alternativ - död eller inte död, på eller av, noll eller ett. Många saker i vår värld följer en bernoullifördelning av något slag, och vi kan även förenkla världen till bernoullifördelningar väldigt lätt: antingen är jag utomhus eller inomhus, sjuk eller frisk, död eller i liv, hungrig eller mätt, och så vidare.\nMed vissa exempel är det lättare att se att fördelningen förmodas än med andra. Att vara död eller inte är en ganska binär sak - du kan inte riktigt vara “typ, så där liksom död”, alltså något mellan död och i liv. Men du kan vara mer eller mindre hungrig, mer eller mindre sjuk, mer eller mindre rik. Vi kan alltså skilja mellan flera olika sorters variabler: binära, kategoriska eller diskreta, och kontinuerliga. Dödhet är binärt3, men vi kan även förlänga skalan till diskreta kategorier: död, dödligt sjuk, sjuk, frisk.3 För att förenkla; en biolog har säkert en mer precis definition på liv än vad vi jobbar med här.\nMed flera kategorier använder vi inte längre bernoullifördelningen, utan en mer generell variant av den: binomialfördelningen."
  },
  {
    "objectID": "statistisk-inferens.html#nollhypotessignifikanstestning-nhst",
    "href": "statistisk-inferens.html#nollhypotessignifikanstestning-nhst",
    "title": "6  Statistisk inferens",
    "section": "6.4 Nollhypotessignifikanstestning (NHST)",
    "text": "6.4 Nollhypotessignifikanstestning (NHST)\nFör att testa hur pålitligt vårt resultat är använder vi oss av nollhypotessignifikanstestning (NHST), en filosofi som kombinerar två viktiga tankesätt från vetenskapsteorin: Fishers sannolikhetstestning och Neyman och Pearsons hypotestestning.\n\n6.4.1 Ronald Fisher och att tillreda te\nRonald Fisher föreslog att vetenskapens uppgift är att räkna ut sannolikheter för händelser. Vi kan förstå detta som att vi ska räkna sannolikheten för vår mätning - hur sannolikt är det att vi fick det resultat som vi mätte, t.ex. könsfördelningen eller medianlönen? Fisher exemplifierade detta genom tankeexperimentet om tetillredning.\nI tedryckens lovade land, Storbritannien, har det länge funnits en myt om att vissa människor kan känna igen huruvida en tekopp har tillagts genom att lägga i mjölk före tevatten, eller tevatten före mjölk. Kan vi testa detta empiriskt? Jo, säger Fisher! Vi låter en person som påstår sig kunna skilja på dessa tillredningssätt smaka på ett antal tillagda te. Hen4 visste inte i vilken ordning tedrycken hade tillretts.4 I ursprungsexemplet var det tal om en kvinna, men könet är inte relevant för detta exempel.\n[FUNDERA UT NÅGOT NYTT HÄR.]\n\n\n6.4.2 Neyman, Pearson, och hypoteser\nDen andra delen av NHST baserar sig på tanken om hypoteser, av Neyman och Pearson. Det finns fyra grundläggande steg i hypotesprocessen:\n\nStäll en fråga av något slag.\nFormulera en alternativ hypotes, \\(H_1\\), som representerar ett positivt svar till frågan.\nFormulera en nollhypotes, \\(H_0\\), som representerar ett nollresultat för frågan.\nUtför ett test som kan bevisa nollhypotesen fel.\n\nProcessen åskådliggörs bäst med ett exempel. Min partner har märkt att det tycks finnas väldigt många metalheads (personer som lyssnar på metallmusik) inom naturvetenskaperna i Finland, men jag har inte märkt någon större mängd av dem inom samhällsvetenskaperna. Frågan vi ställer är därmed: Finns det fler metalheads inom naturvetenskaperna än inom samhällsvetenskaper?\nFrån denna fråga kan vi formulera två hypoteser:\n\n\\(H_1\\): Det finns fler metalheads inom naturvetenskaperna än inom samhällsvetenskaperna.\n\n\n\\(H_0\\): Det finns lika många metalheads inom naturvetenskaperna och samhällsvetenskaperna.\n\nDet är viktigt att märka, att nollhypotesen är inte inversen av den alternativa hypotesen - ett negativt förhållande är också ett förhållande. Vi vill jämföra med avsaknaden av förhållande.\nHärifrån kunde vi nu formulera ett test som kan bevisa nollhypotesen fel. Vi kan utföra en enkätstudie på alla natur- och samhällsvetare i Finland, och fråga dem vilken musikstil de främst lyssnar på. Sedan kan vi jämföra graden av metallyssnare inom bägge vetenskapsområde. Om de resulterande graderna inte är desamma, så kan vi förkasta nollhypotesen: det finns olika mängder metalheads inom respektive vetenskapsgren, så då kan det logiskt inte vara så att det finns lika många i bägge. Med andra ord:\n\\[ H_0: \\frac{n_\\text{metall i naturvetare}}{n_\\text{naturvetare}} =\n\\frac{n_\\text{metall i samhällsvetare}}{n_\\text{samhällsvetare}}\n\\] \\[\nH_1: \\frac{n_\\text{metall i naturvetare}}{n_\\text{naturvetare}} >\n\\frac{n_\\text{metall i samhällsvetare}}{n_\\text{samhällsvetare}}\n\\]"
  },
  {
    "objectID": "statistisk-inferens.html#nhst---att-kombinera-hypoteser-och-sannolikheter",
    "href": "statistisk-inferens.html#nhst---att-kombinera-hypoteser-och-sannolikheter",
    "title": "6  Statistisk inferens",
    "section": "6.5 NHST - att kombinera hypoteser och sannolikheter",
    "text": "6.5 NHST - att kombinera hypoteser och sannolikheter\nUtifrån de två presenterade tankesätten får vi nollhypotessignifikanstestning, eller NHST:\n\nSkapa en alternativ hypotes och en nollhypotes utifrån en frågeställning.\nEstimera ett test som kan bevisa nollhypotesen falsk.\nRäkna sannolikheten för att få ett så extremt test eller större om nollhypotesen vore sann.\nOm sannolikheten är tillräckligt liten: förkasta nollhypotesen.\n\nHur räknar man då sannolikheten? Här kommer fördelningarna till hjälp! Om vi förmodar att vår urvalsprocess följer en viss fördelning, t.ex. normalfördelningen, så kan vi kolla var på fördelningskurvan vår mätta skillnad (skillnaden i graden av metalheads mellan naturvetenskaper och samhällsvetenskaper) ligger. För att göra detta måste man dock ofta standardisera det mätta värdet enligt en eller annan metod. Vi hanterar dessa metoder senare i boken, så låt oss nu bara säga att skillnaden, efter att den har blivit standardiserad, är 1 (på en skala av ungefär -4 till 4). Vi kan rita upp en graf som visar visuellt hur stor sannolikhet ett värde på 1 eller mera är på en normalfördelning:\n\nlibrary(ggplot2)\nggplot(data.frame(x=c(-4, 4)), aes(x=x)) + stat_function(fun=dnorm) + geom_area(stat=\"function\", fun=dnorm, fill=\"grey\", xlim = c(1,4)) + xlab(\"Skillnad\") + ylab(\"Sannolikhet\")\n\n\n\n\nNormalfördelning med svärtat område [1;4].\n\n\n\n\nHur vi exakt beräknar sannolikheten är aningen för komplext för att hantera i denna bok; i praktiken låter vi R räkna den för oss. Resultaten är en siffra, kallat ett p-värde, som varierar mellan 0 och 1 och berättar för oss sannolikheten att få vårt värde eller större om nollhypotesen vore sann."
  },
  {
    "objectID": "statistisk-inferens.html#felgrader-typ-i-och-ii-fel",
    "href": "statistisk-inferens.html#felgrader-typ-i-och-ii-fel",
    "title": "6  Statistisk inferens",
    "section": "6.6 Felgrader: Typ I och II fel",
    "text": "6.6 Felgrader: Typ I och II fel\nFöre vi går in för att förstå olika osäkerhetsmått är det bäst att klargöra vad det är som vi mäter. Vi har redan diskuterat noll- och alternativa hypoteser, men vi kan ställa upp en synnerligen enkel tabell av det vi mäter:\n\n\n\n\n\n\n\n\n\nSant i verkligheten\nFalskt i verkligheten\n\n\nSant i mätningen\nhit vill vi nå!\nTyp I fel (\\(\\alpha\\))\n\n\nFalskt i mätningen\nTyp II fel (\\(\\beta\\))\ndetta är också okej!\n\n\n\nVi vill att vår mätning motsvarar verkligheten; om skillnaden återfinns i mätningen, ska den helst återfinnas i verkligheten, och om skillnaden inte finns i mätningen, ska den helst inte heller finnas i verkligheten. Det kan dock gå ‘fel’ på två sätt:\n\nVi mäter en skillnad som inte finns i verkligheten. Detta kallas typ I fel eller \\(\\alpha\\) (grekiska alfa).\nVi mäter avsaknaden av skillnad, trots att den finns i verkligheten. Detta kallas typ II fel eller \\(\\beta\\) (grekiska beta).\n\nTyp I fel är detsamma som vårt p-värde: det är sannolikheten att finna vår skillnad (eller större) om den inte finns i verkligheten, alltså att nollhypotesen är sann. Ju lägre gräns vi håller på typ I fel, desto säkrare kan vi vara på våra resultat om sannolikhetstestet underskrider gränsen.\nTyp II fel är ‘motsatsen’ av typ I fel, sannolikheten att vi mäter ett nollresultat då det faktiskt finns en skillnad. Inversen av typ II felet, \\(1-\\beta\\), kallas för statistisk styrka, och mäter sannolikheten att finna sanna positiva. Denna vill vi hålla så hög som möjligt - vi vill ju inte missa skillnader då de faktiskt finns!\nDet finns dock ett förhållande mellan Typ I fel och Typ II fel: när \\(\\alpha\\) stiger så sjunker \\(\\beta\\), och när \\(\\beta\\) stiger så sjunker \\(\\alpha\\). Detta betyder att när graden av typ I fel faller (bra!), så stiger graden av typ II fel (dåligt!), vilket innebär att den statistiska styrkan faller (också dåligt!). Omformulerat: När vi minskar gränsen på vad som tas som pålitligt (och därmed ökar pålitligheten av resultat som uppfyller vårt krav), så ökar vi graden av sanna positiva som vi kastar bort. Det är alltså inte nödvändigtvis fallet att vi vill ha ett så lågt p-värde som möjligt, eftersom det ofta innebär att vi börjar kasta ut barnet med badvattnet."
  },
  {
    "objectID": "introduktion-till-r-miljön.html#installera-r-och-rstudio",
    "href": "introduktion-till-r-miljön.html#installera-r-och-rstudio",
    "title": "7  Introduktion till R-miljön",
    "section": "7.1 Installera R och RStudio",
    "text": "7.1 Installera R och RStudio\nFör att använda R (och följa med i boken) måste du först installera R. Utöver detta rekommenderar jag varmt att du installerar ett IDE (eng. Integrated Development Environment). I denna bok använder jag RStudio, och därför ger jag instruktionerna för att installera RStudio. Du kan fritt välja ditt eget IDE, men märk då att den visuella utläggningen (samt några verktyg) kan se annorlunda ut eller saknas. R i sig är detsamma oavsett vilket IDE du väljer.\n\n7.1.1 Installering av R\nDu finner den nyaste installationen av R på R-projektets hemsida. Vid skrivande stund är den nyaste versionen 4.2.2, och denna bok är skriven med version 4.2.2.\n\n\nbase::getRversion(): Returnerar versionen av R du har aktiv.\nR är decentraliserat, vilket bl.a. innebär att det inte finns en endaste plats att ladda ned språket från. Istället laddar vi R (och senare R-resurser) från CRAN (eng. Comprehensive R Archive Network). CRAN uppdateras kontinuerligt till servrar upprätthållna av universitet runtom världen. Det betyder att i sällsynta fall kan en specifik CRAN-server vara “efter” i uppdateringarna, men oftast innebär detta enbart att du måste välja vilken server du laddar R-relaterade resurser ned från.\nInget universitet i Finland upprätthåller CRAN vid tillfället, men t.ex. Umeå universitet har en uppdaterad CRAN-server.\nEfter att du har valt CRAN-server, välj R-installation enligt versionen du vill ha (oftast den nyaste stabila; undvik beta-versioner om du inte har ett explicit behov för dem), och välj ditt operativsystem. Windows-, Mac- och Linux-system kan alla använda R - och använder du Linux, kanske du redan har en installation på din dator. Jag använder Windows, så exempelbilderna i denna bok kommer att härstamma från en Windows-installation.\nInstallera R enligt installationsfilens instruktioner. Om du åker fast kan du finna noggrannare instruktioner på CRAN-serverns sidor.\nOm du vill kan du köra igång med programmering bara med R-installationen. Jag rekommenderar dock att du även installerar ett IDE, t.ex. RStudio nedan, eftersom det stort underlättar arbetet (du kommer att inse varför).\n\n\n7.1.2 Installering av RStudio\nRStudio är ett öppet IDE för R-språket. Du finner den nyaste versionen av RStudio på projektets hemsida. Navigera dig till installationslänken. När nätsidan ber dig att installera R kan du fnissa för dig själv - det har du redan gjort! - och bara välja att ladda ned installationsfilen för RStudio för ditt operativsystem.\n\n\n7.1.3 Navigering i RStudio\nÖppna RStudio när installationen är klar. Du kommer att mötas av en syn ungefär lik denna:\n\n\n\nFigure 7.1: RStudio i sin spektakulära helhet\n\n\nProgrammet är uppdelat ungefär i fem delar: högst uppe över hela fönstret finner du en navigeringsbalk med menyer så som File, Edit, Code, och så vidare. Sedan finner du fyra fönsteraktiga rutor, alla med en massa text, filer, tabbar och så vidare. Vi går igenom de viktigaste kort nedan.\n\n\n\nFigure 7.2: Navigeringsbalken\n\n\nI övre vänstra hörnet lever dina filer. Märk, att om du öppnar RStudio för första gången, så finns det inga filer här. Med andra ord finns inte heller fönstret. Men ingen fara! När du skapar dina första R-filer kommer de att synas här, och din vy kommer att se ut som på bilden.\n\n\n\nFigure 7.3: Filområdet\n\n\nI Figure 7.3 har jag öppet fyra filer relaterade till denna bok: _bookdown.yml, introduktion-till-r-spraket.Rmd (detta kapitel), _output.yml och statistisk-inferens.Rmd. R-kod kan skrivas i en fil för att sedan köras (eng. execute). Du kan skapa R-filer fritt, med en regel: om du inte importerar filen som ett paket (se kapitel XXX), så kan du inte köra kod från en fil om inte filen är öppen och aktiv i rutan. Det är bra att lära sig att samla skriven kod i filer, så att du kan reproducera ditt arbete i ett senare skede.\nI nedre vänstra hörnet finner du verktyg relaterade till att köra kod, så som visas i Figure 7.4.\n\n\n\nFigure 7.4: Körningsområdet\n\n\nTabben Console agerar som ett slags live-fönster för att köra kod: du kan skriva direkta kommando i konsolen och trycka på Enter för att köra dem i realtid. Allt du kan göra genom kod i filerna kan du även göra i konsolen, men det blir tungt att köra komplexa kodfiler rad för rad i konsolen. Konsolen är dock användbar när du testar saker, eftersom du kan snabbt köra om kommando: använd PIL UPP och PIL NED för att återskapa kommando som du kört senast!\nTabben Terminal kör ditt operativsystems egna kodterminal. I denna kurs behöver du inte den, men om du kan använda terminalen eller vill jobba med t.ex. Git eller annan versionskontroll, kan terminalen vara av god användning.\nTabben Render beskriver vad som görs när du kör en fil. Här ser du bl.a. varningar och felmeddelanden (du kommer att se dessa ofta!), samt annan text och debugging som du kanske gör.\nTabben Background Jobs används om du gör asynkrona kodkörningar - något väldigt överkurs för en nybörjare i statistisk data-analys.\nI övre högra hörnet finns några verktyg för att undersöka vad som finns öppet i din R-instans. Instanser är aktiva “versioner” av R. Du kan tänka dig att varje gång du öppnar R, öppnar du en ny papplåda inom vilken du kan stapla data, ändra på variabler, skapa grafer, jobba med olika paket, osv. Då du stänger R tömmer du lådan och packar den undan.\n\n\n\nFigure 7.5: Instansområdet\n\n\nUnder tabben Environment finner du dina öppna variabler. Då du öppnar RStudio är rutan kanske tom. Men pröva på att köra nedanstående kod i konsolen (klipp och klistra kodtexten i konsolen och tryck Enter för att köra):\n\ndata <- c(1:5)\n\n\n\nbase::c(): Skapar en vektor av givna data.\nOm du körde den rätt, så borde det dyka upp en variabel med namnet data i Environment-rutan. Grattis, du har skapat din första variabel! Tabben History berättar åt dig vilka kodsnuttar som har körts senast, vilket kan vara nyttigt om du testar funktioner i konsolen. De övriga tabbarna är inte nödvändiga för vårt bruk, men du kan alltid undersöka manualen för RStudio om du är intresserad.\nI nedre högra hörnet finner du filer, grafik och hjälpmenyn, bland annat. Dessa är väldigt användbara, och du kommer ofta att återvända till detta hörn.\n\n\n\nFigure 7.6: Produktområdet\n\n\nTabben Files visar dig filstrukturen av den aktiva arbetskatalogen (eng. working directory). I detta skede kanske du ser filstrukturen av R-installationens katalog. Vi återkommer till hur du byter denna arbetskatalog till mappen där du (förhoppningsvis) sparar dina projektfiler.\nTabben Plots innehåller din grafik. Just nu är den tom, men ändra på saken genom att köra följande kod i konsolen:\n\nhist(rnorm(100))\n\n\n\n\n\n\ngraphics::hist(): Skapar ett histogram av givna data.  stats::rnorm(n): Drar n antal urval från en normalfördelning.\nNu borde du se en fin graf med namnet Histogram of rnorm(100) i rutan1. Grattis - det var din första graf!1 Denna bok visar exempel på vad som borde synas när jag skriver in grafisk kod. Du kan alltså jämföra bilden du skapade i RStudio med bilden i boken, för att kolla att du har kört allt rätt.\nTabben Packages visar alla paket du har installerat på din dator, samt ifall du har aktiverat paketet i din R-instans. Detta förklaras i ett senare kapitel.\nTabben Help visar dig hjälpfiler och dokumentation om du ber om det. Detta är ytterst viktigt! Du kommer väldigt ofta att finna dig i en situation där du vet att en funktion finns, men du vet inte hur den fungerar. Med förtecknet ? kan du söka efter förklaringar i R-dokumentationen: ?hist öppnar hjälpsidan för den inbyggda R-funktionen för att skapa histogram. I början kan hjälpsidorna vara svåra att läsa, men du finner snabbt att de alltid är strukturerade på ungefär samma sätt. Vi diskuterar detta mera i nästa underkapitel.\n\n\n?func: Visar hjälpsidorna för funktionen func().  ??func: Söker hjälpsidorna genom för dokument som nämner func.\nTabbarna Viewer och Presentation används inte i denna bok."
  },
  {
    "objectID": "introduktion-till-r-miljön.html#ställ-in-din-r-miljö",
    "href": "introduktion-till-r-miljön.html#ställ-in-din-r-miljö",
    "title": "7  Introduktion till R-miljön",
    "section": "7.2 Ställ in din R-miljö",
    "text": "7.2 Ställ in din R-miljö\nFör att kunna jobba flytande med noggrann koll på var alla dina filer finns behöver du göra några preliminära inställningar i RStudio.\nDet första du gör är att du skapar ett R-projekt:\n\nI navigeringsbalken, gå till File > New Project...\nVälj New Directory\nVälj New Project\nGe ditt projekt ett namn under Directory Name, t.ex. “regressionsanalys” då du övar regression.\nVälj var på din hårdskiva R ska spara alla filer relaterade till projektet. Skapa alltid en ny mapp för nya projekt!\n\nDin filstruktur blir nu synlig i Files-tabben i nedre högra hörnet, och innehåller troligen enbart din projektfil.\n\n\nVar är mina analyser?!\n\nOm du märker senare i ditt arbete att du inte längre minns var dina filer har blivit sparade på din dator, kan du använda dig av funktionen getwd(). Genom att köra den i konsolen får du reda på filadressen.\nDu kan även ändra adressen senare, genom funktionen setwd(). Märk dock att detta inte flyttar över alla dina nuvarande filer, och kan därmed orsaka mycket strul och bekymmer.\n\n\n\nbase::getwd(): Returnerar filadressen för nuvarande arbetsmiljö.  base::setwd(path): Ställer in filadressen för arbetsmiljön till path.\nTill näst ska du skapa en R-fil. Detta blir filen som du skriver din kod i för att sedan processera med datorn och köra. Du kan skapa en ny R-fil på tre sätt:\n\nFile > New File > R Script\nIkonen för ny fil strax under File-menyn > R Script\nTryck Ctrl + Shift + N\n\nGenast efter skapande ska du komma ihåg att spara filen med ett deskriptivt namn:\n\nFile > Save\nIkonen för att spara strax under View-menyn\nTryck Ctrl + S\n\nAnefter att du skapar nya filer dyker de upp i Files-tabben i nedre högra hörnet, och du kan öppna dem genom att klicka på dem. Du kan ha flera filer öppna samtidigt, men var noggrann med vilken du editerar och kör som bäst.\nFör att köra kod kan du göra något av följande:\n\nFör en kodrad: Klipp och klistra kodraden från kodfilen till konsolen och tryck Enter.\nMarkera (måla över) en eller flera kodrader i kodfilen och tryck på Run-knappen i kodfönstrets övre högra hörn. Alternativt, tryck Ctrl + Enter.\nCode > Run Region > Run All för att köra hela kodfilen i en gång. Alternativt, tryck ´Alt + Ctrl + R`.\n\n\n\nKonsolen och kodfiler\n\nKonsolen och kodfilerna lever i skilda världar. Du kan tänka dig kodfilen som en receptbok2 och konsolen som ditt kök. Bara för att du har ett mumsigt recept på vegetarisk lasagne betyder inte att maten finns färdig i ditt kök!2 Fysiska versioner av receptbloggar.\nAtt skriva kod är skilt från att köra kod: när du skriver ned koden händer ännu inget. Först när du trycker på ´Run` skickas koden till konsolen som omtolkar den till maskinspråk bakom kulisserna, utför alla instruktioner, och spottar ut de värden du bett den att ge (inklusive felmeddelande).\nDetta betyder, att om du inte har kört en viss rad kod sedan du öppnade konsolen (varje gång du stänger RStudio nollställs konsolen!), så vet inte konsolen om att den raden kod finns till. Om du får fel som tyder på att ett visst värde inte existerar, kan det alltså bero på att du kanske definierat värdet i din kodfil, men glömt att köra definitionen i konsolen.\n\nNär du stänger RStudio tömmer den sitt minne och “glömmer” bort allt som inte har sparats. Om du sparat dina kodfiler är det ingen fara, du kan öppna dem på nytt. Däremot glömmer R bort alla objekt du kallat i konsolen (mera om vad dessa är senare), så du måste upprepa dina steg på nytt när du börjar om nästa gång. Detta är en bra sak, trots att det låter arbetsdrygt! Det tvingar dig att reproducera dina resultat om och om igen, vilket säkrar att du vet vad du gör, resultaten stannar detsamma, och det faktiskt går att reproducera dina analyser.\nRStudio kan spara arbetsmiljön i formen av en .Rdata-fil, och när du stänger RStudio frågar den dig om du vill göra så. Detta är dålig praxis och rekommenderas inte, eftersom du riskerar att blanda ihop projekt och data om du byter mellan olika analyser."
  },
  {
    "objectID": "grundläggande-kunskaper-i-programmering.html#objekt-och-datatyper",
    "href": "grundläggande-kunskaper-i-programmering.html#objekt-och-datatyper",
    "title": "8  Grundläggande kunskaper i R-programmering",
    "section": "8.1 Objekt och datatyper",
    "text": "8.1 Objekt och datatyper\nEtt objekt kan hålla ett värde. Dessa kallas även ibland för “variabler”, men för att skilja från statistiska variabler håller jag mig till termen “objekt”.\nObjekt kan skapas med hjälp av en speciell operator: <- (uttalas “får” eller “tilldelas”). Du skapar ett objekt genom att ge det ett unikt namn, rikta pilen mot objektet, och skriv vad som objektet ska innehålla:\n\nx <- 12\n\nDetta kommando skapar ett objekt med namnet x, och tilldelar det värdet 12. Om du kör koden, märker du att inget tycks hända. Objektskapande gör inget annat än att det skapar objektet - om du vill skriva ut innehållet i konsolen måste du kalla objektet:\n\nx\n\n[1] 12\n\n\nObjekt kan omskrivas genom att tilldela dem nya värden. Det senaste tilldelade värdet är det som objektet innehåller:\n\nx <- 12\nx <- 47\nx\n\n[1] 47\n\n\nObjekt har en viss klass. I R görs detta implicit, till skillnad från vissa andra programmeringsspråk där du måste säga åt datorn: “Dator! Detta är objekt x, och den har klassen numeric!” R är lite smartare än så, och känner automatiskt igen vilken klass objektet har. Det finns fyra1 grundläggande klasser:1 Enligt ?typeof() finns det många fler än så, och skillnaden mellan klasser och datatyper (eng. classes and data types) är lite osäker i R. Förenklingen är för pedagogiskt syfte.\n\ncharacter: Text, t.ex. \"a\", \"Här är en mening!\", \"klsjdfjjsdfksdnf\"\nnumeric: Siffror, t.ex. 2.5, 96, 3068.12. Notera att R använder punkt som decimalmarkerare!\nlogical: Logiskt värde, t.ex. TRUE, FALSE, NA. Kallas även booleanska värden.\nfactor: Speciell texttyp med inbyggd ordning. Dessa hanteras i ett eget kapitel.\n\nDu kan granska klassen av ett objekt eller data genom att ge det som argument till funktionen class():\n\na <- 12\nclass(a)\n\n[1] \"numeric\"\n\nb <- FALSE\nclass(b)\n\n[1] \"logical\"\n\nc <- \"Äpple\"\nclass(c)\n\n[1] \"character\"\n\n\n\n\nbase::class(x): Returnerar klassen för objektet x."
  },
  {
    "objectID": "grundläggande-kunskaper-i-programmering.html#manipulering-av-objekt",
    "href": "grundläggande-kunskaper-i-programmering.html#manipulering-av-objekt",
    "title": "8  Grundläggande kunskaper i R-programmering",
    "section": "8.2 Manipulering av objekt",
    "text": "8.2 Manipulering av objekt\nObjekt är gjorda för att manipuleras på ett sätt eller annat. Du kan addera, subtrahera, multiplicera och dividera objekt, men även utföra oändligt mer komplexa funktioner på objekt.\nDet finns sex grundläggande matematiska operatorer som du kan använda:\n\n+: Addition\n-: Subtraktion\n*: Multiplikation\n/: Division\n^: Exponentiering\n%%: Modulus\n\nDen sistnämnda är du kanske obekant med: modulus ger resten efter att två tal har dividerats:\n\n5 %% 2\n\n[1] 1\n\n7.5 %% 2.5\n\n[1] 0\n\n36.15 %% 2.34\n\n[1] 1.05\n\n\nDu kan även använda operatorerna på objekt - men märk väl vad som oftast händer när du blandar dataklasser:\n\na <- 12\nb <- 3\nc <- \"f\"\na+b\n\n[1] 15\n\na*b\n\n[1] 36\n\na+c\n\nError in a + c: non-numeric argument to binary operator\n\n\n\n\nTolkning av fel\n\nProgrammering är en process av att pröva, misslyckas och pröva igen. Det innebär att du kommer att bemöta väldigt många fel, ofta bemärkta med skrämmande röd text och ordet “Error”. Var inte rädd!\nDebugging är processen där du utreder vad som gick fel. Varför kastade programmet detta fel? Vad händer om jag ändrar på inputen som orsakade felet? Kan jag hitta i vilket skede felet uppstod?\nJag kan inte ge dig direkta instruktioner på hur du löser fel, men här är några råd:\n\nTesta din kod ofta! Skriv inte 500 rader kod i en session utan testning, eftersom det gör felhittande svårare.\nOm du hittar ett fel, börja med att läsa felet. Det kan låta lustigt, men de allra flesta användarna är… dåliga, ska vi säga, på att faktiskt läsa felmeddelandet! Läs genom och se om du kan förstå det.\nEfter ett fel, testa din nya kod en rad åt gången. Var sker felet? Detta hjälper dig lösa det.\nOm du inte kan lösa det, klipp och klistra felmeddelandet i en sökmotor och börja leta. Programmering är till stor del informationssökning, och det kan vara att flera personer har haft samma problem. Ibland är det ditt fel, ibland är det paketskaparens fel, ibland måste du bara starta datorn om och ta ett djupt andetag.\nBli inte arg på dig själv när du skriver fel! De mest professionella programmerarna gör “nybörjarmisstag” (avsaknaden av slutparentes, tyrckfel, logikfel) konstant. Ett proffs utmärks av att de inte ger upp vid problem."
  },
  {
    "objectID": "grundläggande-kunskaper-i-programmering.html#funktioner",
    "href": "grundläggande-kunskaper-i-programmering.html#funktioner",
    "title": "8  Grundläggande kunskaper i R-programmering",
    "section": "8.3 Funktioner",
    "text": "8.3 Funktioner\nFunktioner är som matlagningsrecept. De är färdiga instruktioner som du kan repetera om och om igen, med olika inputs och outputs. Du har redan stött på ett antal funktioner: class() i detta underkapitel, hist() och c() i det föregående underkapitlet.\nEn funktion tar formen funktion(argument, ...). En funktion har därmed ett namn, följt av parenteser som kan innehålla argument. Argument är data som du överräcker (eng. pass) åt funktionen. Vissa funktioner tar inga argument, andra måste ta argument. Du kan alltid ta reda på vad en funktion gör med hjälpfunktionen: ?funktion.\nDu kan även definiera dina egna funktioner, men detta är inte nödvändigt för grundläggande kunskaper i statistisk programmering i detta skede. Om du är intresserad, vänd dig t.ex. till Intro2r-boken.\nDet finns otroligt många funktioner, och jag kan inte rada upp alla. Istället kommer jag att sakta introducera olika funktioner genom boken. Var liberal med din användning av hjälpfunktionen - du kommer att behöva den!"
  },
  {
    "objectID": "grundläggande-kunskaper-i-programmering.html#vektorer",
    "href": "grundläggande-kunskaper-i-programmering.html#vektorer",
    "title": "8  Grundläggande kunskaper i R-programmering",
    "section": "8.4 Vektorer",
    "text": "8.4 Vektorer\nAtt ha en datapunkt i ett objekt är helt okej och användbart, men vi jobbar ju med större mängder data. Vi vill ha flera datapunkter i ett objekt! Detta kan vi åstadkomma enkelt med vektorer. En vektor konstrueras med funktionen c() (från engelskan concatenate).\n\nvektor <- c(1, 2, 3, 4)\nvektor\n\n[1] 1 2 3 4\n\n\n\n\nbase::c(x): Konstruerar en vektor av givna data x.\nVektorer har samma klass som de värden som tillhör vektorn - ovanstående vektor är av klassen numeric - vilket även betyder att klasser inte kan blandas i en vektor. Om du trots allt försöker göra det, så tvingar vektorn alla värden till en klass som kan innehålla alla. Detta är oftast character:\n\nvektor <- c(\"a\", 2, TRUE)\nclass(vektor)\n\n[1] \"character\"\n\n\nVektorer kan indiceras. Detta betyder att du kan hämta N:te värdet i vektor med klamrar:\n\nvektor[1]\n\n[1] \"a\"\n\nvektor[2]\n\n[1] \"2\"\n\nvektor[4]\n\n[1] NA\n\n\nMärk, att om du söker ett index som inte finns (index 4 i ovan exempel), så returnerar R värdet “NA”. Detta är ett speciellt logiskt värde som betyder “Hej! Här finns ju inget!”.\nFörutom explicita indexvärden kan du även ge intervall, vektorindex och logiska index:\n\nvektor[1:3] # Hämtar värden vid position 1 till 3.\n\n[1] \"a\"    \"2\"    \"TRUE\"\n\nvektor[c(1,3)] # Hämtar värden 1 och 3.\n\n[1] \"a\"    \"TRUE\"\n\nvektor[vektor == \"a\"] # Hämtar alla värden som motsvarar logiska testet.\n\n[1] \"a\"\n\n\nMed booleanska uttryck kan du även skapa mer komplexa index: vektor > 1 & vektor != 4 söker alla värden över ett, förutom fyra. De booleanska basuttrycken är:\n\n&: “och”, både A & B måste vara sanna.\n|: “eller”, antingen A | B måste vara sann, eller bägge.\n!: “inte”, ger inversen av objektet (om A = TRUE, så !A = FALSE).\n<: “mindre än”, testar för att A är mindre än B\n>: “större än”, testar för att A är större än B\n==: “är lika med”, testar för att A är samma som B\n!=: “är inte lika med”, testar för att A är olika från B\n<=: “är mindre än eller lika med”, testar för att A är antingen mindre än eller samma som B\n>=: “är större än eller lika med”, testar för att A är antingen större än eller samma som B\n\n\n\nIndicering och fel-med-ett\n\nR avviker från många andra programmeringsspråk genom att R räknar från 1, inkluderande. Detta innebär två saker: Att index alltid börjar från 1, och att en indexintervall innehåller den definierade ändan. Indexet [4] hämtar det fjärde elementet, intervallet [2:5] hämtar det andra, tredje, fjärde och femte elementet.\nDet är mycket vanligt att programmerar gör fel-med-ett (eng. off-by-one error). Detta innebär att du misstolkar vilka element som hämtas eller behandlas, och ökar eller minskar intervallets storlek med ett i misstag. Var alltså alltid noggrann med dina intervall, och dubbelkolla att rätt mängd element har hämtats eller rätt mängd iterationer (upprepningar) har gjorts!"
  },
  {
    "objectID": "grundläggande-kunskaper-i-programmering.html#matriser-arrays-listor-och-dataramar",
    "href": "grundläggande-kunskaper-i-programmering.html#matriser-arrays-listor-och-dataramar",
    "title": "8  Grundläggande kunskaper i R-programmering",
    "section": "8.5 Matriser, arrays, listor och dataramar",
    "text": "8.5 Matriser, arrays, listor och dataramar\nVektorer och enkla värden (även kallat scalars) är exempel på datastrukturer. Eftersom du ska lära dig statistik i R, är det viktigt att du lär dig hur du kan forma och omforma dina data. Det finns fyra ytterligare datastrukturer i basinstallationen av R som du bör känna till: matriser, arrays, listor och dataramar\n\n8.5.1 Matriser\nMatriser är tvådimensionella versioner av vektorer. En matris har kolumner och rader, och varje kombination av kolumn- och radposition utgör en cell. Du kan skapa matriser med funktionen matrix(data, nrow = x, ...)\n\nmatris_kol <- matrix(1:9, nrow=3)\nmatris_kol\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nmatris_rad <- matrix(1:9, nrow=3, byrow=TRUE)\nmatris_rad\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n\nbase::matrix(x, nrow, ncol, byrow, dimnames): Definierar en datamatris. x ger data, nrow och ncol ger mängden rader och kolumner, byrow väljer om matrisen ska fyllas per rad eller per kolumn, och dimnames ger namn åt raderna och kolumnerna i en lista med två element.\nArgumentet byrow=TRUE tvingar matrisen att lägga in data enligt rader (börja med kolumn 1, rad 1, sedan kolumn 1, rad 2, osv.), medan byrow=FALSE eller tomt argument lägger in data enligt kolumner.\nDu kan söka data från matriser genom att ge ett indexpar, där det första indexet är radindex och det andra är kolumnindex:\n\nmatris_kol[1,3]\n\n[1] 7\n\n\nDu kan även ge namn åt raderna och kolumnerna istället för ordningstal. Detta gör sökning av data något lättare. Du gör det genom funktionerna rownames(matris) och colnames(matris):\n\nrownames(matris_kol) <- c(\"A\", \"B\", \"C\")\ncolnames(matris_kol) <- c(\"Å\", \"Ä\", \"Ö\")\nmatris_kol\n\n  Å Ä Ö\nA 1 4 7\nB 2 5 8\nC 3 6 9\n\nmatris_kol[\"A\",\"Ö\"]\n\n[1] 7\n\n\n\n\nbase::rownames(M): Returnerar eller bestämmer radnamnen för matrisen M, beroende på riktning.  base::colnames(M): Returnerar eller bestämmer kolumnnamnen för matrisen M, beroende på riktning.\n\n\n8.5.2 Arrays\nArrays är mångdimensionella versioner av matriser. Arrays är därmed mycket lika som matriser, men fungerar i högre dimensioner och skapas med funktionen array(data, dim = x, ...):\n\nmin_array <- array(1:18, dim = c(3, 3, 2))\nmin_array\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\nmin_array[1,3,2]\n\n[1] 16\n\n\n\n\nbase::array(x, dim, dimnames): Definierar en array. x ger data, dim ger storleken på varje dimension i en vektor, och dimnames ger ger namn åt dimensionerna i en lista med lika många element som dimensioner.\nArrays fungerar på samma sätt som matriser, men de definieras med argumentet dim och en vektor som beskriver hur många nivåer i varje dimension. Första indextalet hänvisar till raden, andra till kolumnen, tredje till tredje dimensionen. Det blir dock komplext att tänka i flera dimensioner, och därför har det utvecklats bättre verktyg än arrays för detta. Mer om det senare.\n\n\n8.5.3 Listor\nListor är mångdimensionella datastrukturer. Unikt för listor är dock att de kan innehålla flera dataklasser! En lista kan skapas med konstruktorfunktionen list(element_1, element_2, ...):\n\nmin_lista <- list(c(\"Celery Man\", \"Carol Henderson\", \"Dee Vee\"),\n                  c(1962, 1978, 2054),\n                  c(50000, 32000, 0))\nmin_lista\n\n[[1]]\n[1] \"Celery Man\"      \"Carol Henderson\" \"Dee Vee\"        \n\n[[2]]\n[1] 1962 1978 2054\n\n[[3]]\n[1] 50000 32000     0\n\n\n\n\nbase::list(x, all.names, sorted): Definierar en lista. x ger data, antingen som vektorer eller som nyckel-tal-kopplingar; all.names bestämmer huruvida alla data i x ska kopieras i listan eller enbart de som inte börjar med en punkt; och sorted bestämmer huruvida listans element ska sorteras enligt ökande storlek.\nDu kan kalla data från listan med index, eller genom att ge namn åt de olika elementen. Namngivning kan göras i konstruktorfunktionen: list(namn_1 = data_1, namn_2 = data_2, ...) eller i efterhand med names(lista):\n\nnames(min_lista) <- c(\"Namn\", \"Födelseår\", \"Inkomst\")\nmin_lista\n\n$Namn\n[1] \"Celery Man\"      \"Carol Henderson\" \"Dee Vee\"        \n\n$Födelseår\n[1] 1962 1978 2054\n\n$Inkomst\n[1] 50000 32000     0\n\nmin_lista$Namn\n\n[1] \"Celery Man\"      \"Carol Henderson\" \"Dee Vee\"        \n\n\n\n\nbase::names(x): Returnerar namnen för objektet x. Vad som returneras beror på objektet: vektorer returnerar en namnvektor om namnen är definierade, annars NULL. Arrays och matriser returnerar dimnames. Listor returnerar ordningsföljden eller name om namnen är definierade.\nSom du ser kan specifika namngivna element kallas med formen lista$element. Märk väl att namnet ska vara skrivet exakt som det är i listan: $Namn och $namn är två olika elementnamn.\n\n\n8.5.4 Dataramar\nDataramar (eng. data frames) är en speciell datastruktur i R, och kanske den mest användbara av alla dessa (förutom vektorer, som behövs hela tiden). Du kan tänka dig att en dataram är en matris vars varje kolumn är en egen vektor. Detta innebär att varje enskild kolumn bör innehålla samma klass av data, men kolumnerna i sig kan innehålla olika klasser. Kolumn A kan vara text, kolumn B bilder, kolumn C datum, och så vidare.\nDataramar skapas med konstruktorfunktionen data.frame(data):\n\nperson.namn <- c(\"Kasper\", \"Jesper\", \"Jonatan\")\nperson.kön <- c(\"M\", \"M\", \"X\")\nperson.ålder <- c(17, 22, 28)\n\npersoner <- data.frame(namn = person.namn, kön = person.kön, ålder = person.ålder)\npersoner\n\n     namn kön ålder\n1  Kasper   M    17\n2  Jesper   M    22\n3 Jonatan   X    28\n\n\n\n\nbase::data.frame(x, ...): Definierar en dataram. x ger data som värden eller nyckel-tal-kopplingar. För övriga inställningar, se hjälpfunktionen ?data.frame.\nDu kan hämta kolumner från dataramar med indicering samt med namnsökningsoperatorn, t.ex. personer[1] eller personer$namn.\nFör dataramar finns det även ett antal behändiga funktioner för snabba översikter. Funktionen head(data.frame) ger de första sex rader för en snabb insikt i vad som finns i datastrukturen, och funktionen tail(data.frame) gör detsamma för de sista sex raderna. Funktionen str(data.frame) ger en kort summering av dataramens innehåll. Funktionen summary(data.frame) gör det samma, men med statistisk information om en kolumn innehåller siffror.\n\n\nutils::head(df, n): Returnerar de n första raderna i dataramen df. Standardvärdet är 6 rader.\nutils::tail(df, n): Returnerar de n sista raderna i dataramen df. Standardvärdet är 6 rader.\nutils::str(x, ...): Visar strukturen på ett objekt x. Resultatet beror på hurudant objekt som ges och vad det innehåller. För övriga inställningar, se hjälpfunktionen ?str.\nbase::summary(x, maxsum, digits, quantile.type): Ger summerande statistik på objektet x. maxsum ställer in hur många faktornivåer som summeras (i fallande frekvens), digits hur många decimaler som rapporteras, och quantile.type vilken kvantilalgoritm som används (se ?quantile för information om dessa).\n\nmin_dataram <- data.frame(x = round(rnorm(100), digits = 2), y = rep(1:100), z = 5)\nhead(min_dataram)\n\n      x y z\n1 -0.48 1 5\n2  0.81 2 5\n3  0.28 3 5\n4  0.37 4 5\n5  0.70 5 5\n6 -1.27 6 5\n\ntail(min_dataram)\n\n        x   y z\n95   0.18  95 5\n96  -1.13  96 5\n97  -1.26  97 5\n98  -1.03  98 5\n99  -0.62  99 5\n100  1.22 100 5\n\nstr(min_dataram)\n\n'data.frame':   100 obs. of  3 variables:\n $ x: num  -0.48 0.81 0.28 0.37 0.7 -1.27 0.92 1.2 0.76 1.12 ...\n $ y: int  1 2 3 4 5 6 7 8 9 10 ...\n $ z: num  5 5 5 5 5 5 5 5 5 5 ...\n\nsummary(min_dataram)\n\n       x                 y                z    \n Min.   :-2.5900   Min.   :  1.00   Min.   :5  \n 1st Qu.:-0.9125   1st Qu.: 25.75   1st Qu.:5  \n Median :-0.1050   Median : 50.50   Median :5  \n Mean   :-0.1444   Mean   : 50.50   Mean   :5  \n 3rd Qu.: 0.7075   3rd Qu.: 75.25   3rd Qu.:5  \n Max.   : 1.6300   Max.   :100.00   Max.   :5"
  },
  {
    "objectID": "grundläggande-kunskaper-i-programmering.html#tibbles",
    "href": "grundläggande-kunskaper-i-programmering.html#tibbles",
    "title": "8  Grundläggande kunskaper i R-programmering",
    "section": "8.6 Tibbles",
    "text": "8.6 Tibbles\nEn sista datatyp är värd att nämna. Tibbles är en alternativ version av dataramar som fungerar enligt Tidy Data-principen. Detta är en slags designfilosofi utvecklad för R, där alla olika sätt att hantera data (datastrukturer, grafik, kategoriska variabler…) följer samma principer. Detaljerna kan bli klarare för dig när du jobbar med Tidy Data, men principen är att Tidy Data gör arbetet smidigare, mer organiserat och tydligare.\nFör att använda tibbles bör du installera paketet tibble. Vi kommer även att använda andra Tidyverse-verktyg, så om du vill kan du installera hela moderpaketet tidyverse på en gång. Se kapitel XXX om paketinstallation i detta skede, och återvänd här när du är klar.\nEn tibble skapas på samma sätt som en dataram, men med konstruktorfunktionen tibble(data). Om du har en färdig dataram kan du även omvandla den med funktionen as_tibble(dataram)2:2 I versioner av Tibble äldre än 2.0.0 heter funktionen as.tibble().\n\n\ntibble::tibble(x): Definierar en tibble från data x.\ntibble::as_tibble(df): Definierar en tibble från en färdig dataram df.\n\nlibrary(tibble)\nmin_tibble <- as_tibble(min_dataram)\nmin_tibble\n\n# A tibble: 100 × 3\n       x     y     z\n   <dbl> <int> <dbl>\n 1 -0.48     1     5\n 2  0.81     2     5\n 3  0.28     3     5\n 4  0.37     4     5\n 5  0.7      5     5\n 6 -1.27     6     5\n 7  0.92     7     5\n 8  1.2      8     5\n 9  0.76     9     5\n10  1.12    10     5\n# … with 90 more rows\n\n\nTibbles har några inbyggda fördelar (eller skillnader) framöver dataramar:\n\nNär du kallar ett tibbleobjekt skriver den endast ut de tio första raderna, så att du inte i misstag fyller din konsol med tusentals rader och kraschar RStudio. Behöver du flera rader kan du använda funktionen print(data, n = x, width = Inf) för att skriva ut alla kolumner för X rader.\n\n\n\nbase::print(x, ...): Skriver ut objektet x. Inställningarna beror på objektets klass och datatyp. För tibbles kan man bl.a. ge width för mängden kolumner och n för mängden rader.\n\nTibbles ändrar inte dataklassen av värdena som matas in i den. Dataramar har en tendens att omvandla text till faktorer, vilket du kanske inte alltid vill göra - tibbles garanterar att data går in på samma sätt som du matade det in.\nTibbles kräver att hela kolumnens namn skrivs ut när du kallar data, medan dataramar kan tolka halvfärdiga kolumnnamn. Detta kan vara både positivt (skyddar mot misstag) och negativt (orsakar fler fel).\nOm du hämtar en kolumn från en tibble returnerar den alltid en tibble, medan en dataram returnerar en vektor.\nTibbles är strikta med inläggning av nya data i existerande tibbles, och kräver att du ger exakt lika många cellvärden som det finns celler i tibblen. Om du har en tibble med 500 rader, och du ger den en ny kolumn med 450 unika värden, kastar tibblen ett fel. Dataramar gör inte detta, utan upprepar vektorn från början när den nått slutet, ända tills alla rader har blivit fyllda.\n\nTibbles kan användas i så gott som alla situationer där dataramar kan användas, men om en funktion inte tillåter en tibble som argument kan du alltid omvandla tibblen till en dataram med as.data.frame(tibble).\n\n\nbase::as.data.frame(x, ...): Omvandlar objektet x till en dataram. För övriga inställningar, se hjälpfunktionen ?as.data.frame."
  },
  {
    "objectID": "grundläggande-kunskaper-i-programmering.html#sammanfattning",
    "href": "grundläggande-kunskaper-i-programmering.html#sammanfattning",
    "title": "8  Grundläggande kunskaper i R-programmering",
    "section": "8.7 Sammanfattning",
    "text": "8.7 Sammanfattning\nGrunderna i programmering är relativt enkla: definiera objekt och manipulera objekt. Du har lärt dig att göra detta med olika klasser. Kom ihåg att hålla reda på vilken klass varje objekt är, så att du kan förutspå vad som händer i dina manipulationer!\nDen allra största delen av programmering, speciellt i dina statistiska arbeten, sköts av diverse funktioner. Som du redan vet är dessa färdiga recept för att utföra en rad av instruktioner, så att du inte behöver skriva ut varje enskild instruktion för sig.\nDet finns flera olika datastrukturer: du har lärt dig om vektorer, matriser, arrays, listor, dataramar och tibbles. Av dessa är vektorer, dataramar och tibbles troligen de viktigaste för dina ändamål. Vektorer är ett lätt sätt att mata funktioner med multipla värden eller argument, dataramar är behändiga strukturer för att spara dina rådata (och varför inte senare analyser - allt kan sparas i dataramar!), och tibbles är uppstädade versioner av dataramar som fungerar något mera förutsägbart.\nI början kommer du säkert att göra missar med dataklasser och datastrukturer; det är lätt att glömma vilken typ av data som rör sig i ens objekt. Du kommer också att stöta på situationer där en viss funktion inte tar den klass eller struktur av data som du vill ge den. Återvänd till detta kapitel för att förfriska ditt minne, och pröva dig fram med kreativa lösningar. Glöm inte heller Google - chansen att någon annan har haft samma problem som du är väldigt stor!\nI nästa kapitel lär vi oss att skapa grafiska uppställningar av data - något R är speciellt bra på.\n\n\nFunktioner:\n\n\nclass(data): Berättar objektets datatyp.\nmatrix(data, nrow = x, byrow = BOOL, ...): Skapar en matris. Argumentet nrow bestämmer mängden av rader, byrow bestämmer om matrisen fylls per kolumn eller per rad.\nrownames(matris): Definierar radernas namn för en existerande matris.\ncolnames(matris): Definierar kolumnernas namn för en existerande matris.\narray(data, dim = x): Skapar en array. Arrays är mångdimensionella versioner av matriser. Argumentet dim tar en vektor, där varje element står för hur stor dimensionen är. Exempel: array(1:40, dim = c(5, 2, 4)) skapar en tredimensionell array med fem rader, två kolumner, och fyra nivåer av den tredje dimensionen. Exemplet fyller sedan arrayn med siffrorna 1 till 40.\nlist(data): Skapar en lista. Listor kan innehålla många olika datatyper.\nnames(lista): Definierar namnen på elementen i en existerande matris. Efter detta kan du kalla data från listan per namn istället för indexposition.\ndata.frame(data): Skapar en dataram. Du kan ge enskilda vektorer som argument i formen kolumnnamn = vektor.\nhead(dataram): Returnerar de sex första raderna i en dataram. Du kan ge argumentet n = x för att kalla X rader istället.\ntail(dataram): Returnerar de sex sista raderna i en dataram. Samma argument gäller som för head().\nstr(dataram): Returnerar en beskrivning av dataramen.\nsummary(dataram): Returnerar univariata mått på dataramens innehåll. Du kan kalla enskilda kolumner med $-konstruktionen: dataram$x kallar kolumnen x.\ntibble(data): Skapar en tibble. Konstruktionen sker på samma sätt som för dataramar. Tibbles är något enklare och tydligare än dataramar.\nas_tibble(dataram): Omformar en dataram till en tibble.\nas.data.frame(tibble): Omformar en tibble till en dataram."
  },
  {
    "objectID": "r-del-två.html#paket",
    "href": "r-del-två.html#paket",
    "title": "9  R, del två",
    "section": "9.1 Paket",
    "text": "9.1 Paket\nR-språket kan förstås som en bas, lite liknande som grunderna i ett naturligt språk. Du kan göra en hel del med grunderna, uttrycka grundläggande meningar och kanske även klara dig relativt långt i kommunikation. Snabbt når du dock en vägg, där språket inte räcker till. För att utvidga språket lär du dig nya semantiska register, samlingar av ord som alla tillhör något visst område. Du kanske lär dig naturvetenskapliga ord, eller IT-relaterade ord, eller ord om familjelivet.\nPå samma sätt har R olika “register”, vilka kallas för paket. I själva verket är basinstallationen av R uppbyggd av paket. I Packages-tabben i RStudio kan du se vilka paket som finns installerade samt aktiverade i din R-instans. Du kan även hämta en vektor med alla aktiva paket genom att köra kommandot base::loadedNamespaces(). Några viktiga paket som kan hjälpa dig i dina studier är:\n\ndatasets: Ett stort antal olika dataset som du kan använda i övningssyften.\ngraphics: Grundläggande grafikverktyg för datavisualisering.\nforeign: Verktyg för att läsa in data från olika filformater, bl.a. SPSS- och Stata-filer.\nstats: Grundläggande statistiska modeller och testverktyg.\n\n\n\nbase::loadedNamespaces(): Returnerar vektor med alla aktiva paket.\n\n\nMasking\n\nVad händer om du har två funktioner med samma namn från olika paket? Enkelt sagt: R tolkar kommandot som att det tillhör den senare aktiverade.\nLåt oss säga att vi har två paket, A och B, som båda innehåller en funktion med samma namn, f(). Om du aktiverar paket A först och paket B sedan, och kör f(), så kommer R att köra versionen från B. Om du aktiverar B först och A sedan, kör R versionen från A.\nHur kommer man runt detta? Det lättaste sättet är att köra explicita kommandon. Dessa skrivs i formen paket::funktion(). Detta säkrar att du kör funktionen från det paket du vill köra från.\n\n\n9.1.1 Installera och aktivera paket\nPaket laddas ned från CRAN1, på samma sätt som basinstallationen av R. Men du behöver inte gå på nätet och söka efter suspekta .exe-filer - du kan installera paket direkt ur R!1 De kan även laddas ned och byggas från t.ex. GitHub, men vi håller oss till CRAN-paket i denna bok.\nFunktionen install.packages(*paket) låter dig installera paket på två olika sätt. Du kan köra funktionen utan argument install.packages(), vilket öppnar en meny med alla paket du kan installera från CRAN. Detta är dock väldigt många, och det kan vara svårt att hitta paketet du söker efter. Alternativt kan du köra funktionen med paketets namn som argument, t.ex. install.packages(ppcor). Detta exempel installerar paketet ppcor, vilket innehåller funktioner för partiella och delkorrelationer.\n\n\nutils::install.packages(paket): Installerar paketet paket från CRAN. Ber dig välja CRAN-server om den inte redan är vald.\nFör att ladda ned ett paket samt alla de subpaket paketet kräver för att fungera, använd argumentet “dependencies”: install.packages(\"ppcor\", dependencies=TRUE).\nDu behöver endast installera ett paket en gång, så du har inget behov att köra funktionen varje gång du öppnar R. Däremot måste du aktivera ett paket varje gång du vill använda det i en unik R-instans. När du aktiverar ett paket dyker det upp i vektorn som loadedNamespaces() producerar, och då är alla paketets funktioner äntligen användbara.\nDu kan aktivera ett paket i RStudio genom att trycka för checkboxen bredvid paketets namn. I kod kan du aktivera ett paket med funktionen library(paket). Märk att ordningen av paketens aktivering kan påverka vilken funktion som blir aktiv i en situation där du har dubbletter!\n\n\nbase::library(paket): Aktiverar paketet paket. Paketet måste vara installerat först."
  },
  {
    "objectID": "r-del-två.html#data-in-och-ut-ur-r",
    "href": "r-del-två.html#data-in-och-ut-ur-r",
    "title": "9  R, del två",
    "section": "9.2 Data in och ut ur R",
    "text": "9.2 Data in och ut ur R\nKlappa dig själv på axeln, du har kommit långt i din R-inlärning! Till nästa lär vi oss hur du får data in i och ut ur R-miljön. Detta är väldigt viktigt, eftersom du ju behöver dina data för att kunna utföra data-analys.\nGrunderna i att läsa in data är faktiskt väldigt lätta: mina_data <- dataläsningsfunktion(). Med andra ord skapar du ett objekt för dina data, och sedan tilldelar du dina data till objektet genom en dataläsningsfunktion. Hur detta exakt görs beror på typen av dina data. Övningsdatafilen vi använder är av formatet csv, men du kan även läsa in andra typer av data än just det. Ett populärt format är SPSS-filer, så första kapitlet handlar om dem.\n\n9.2.1 SPSS\nEtt mycket vanligt verktyg för statistisk analys bland människovetare är IBM SPSS Statistics. Eftersom vi jobbar i R i denna bok, vill jag välkomna användare av SPSS att byta och se ljuset. Trots detta kommer ändå du, kära läsare, att garanterat bemöta SPSS i en form eller annan i framtiden. Då kan du briljera med hur enkelt det är att läsa in SPSS-data i R, köra dina fina analyser och skapa ännu finare grafiska upplägg, och exportera till olika filformat.\nTidyverse-paketet readr innehåller behändiga verktyg för hantering av SPSS-datafiler, både i formaten sav och por. För läsning av SPSS-filer, använd funktionerna read_sav(path) och read_por(path). Bägge funktion fungerar på samma sätt, ge dem filadressen för filen som ska läsas, samt valfria ytterligare argument. Nedan är några behändiga argument:\n\n\nreadr::read_sav(path, ...): Läser in data från en .sav-fil. För övriga inställningar, se hjälpfunktionen ?read_sav. readr::read_por(path, ...): Läser in data från en .por-fil. För övriga inställningar, se hjälpfunktionen ?read_por.\n\nencoding: Om filen använder en speciell teckenkodning, eller om det inte går att läsa teckenkodningen automatiskt från filen, så kan du specificera den med detta argument.\nuser_na: Huruvida användarspecificerade saknande koder ska kodas till egna faktornivåer (TRUE) eller konverteras till NA (FALSE). Det är rekommenderat att använda NA inom R om du inte absolut måste använda något annat, eftersom de flesta R-funktioner kan automatiskt hantera saknande värden med NA-värdet.\ncol_select: Vilka kolumner/variabler ska hämtas in? Utan detta argument läses alla kolumner in.\nskip: Hur många rader vill du skippa före du börjar läsa in data? Standardvärdet är noll, alltså att funktionen börjar från första dataraden.\nn_max: Största mängden rader som funktionen läser in.\n\nFör skrivning av SPSS-data, använd funktionen write_sav(data, path). Du kan ytterligare ge kompressionsfunktionen som argument, om du vet vad en sådan är och kan använda dem, samt tidszonsdefinitioner.\n\n\nreadr::write_sav(x, path, ...): Skriver in data x till en .sav-fil med namnet och adressen path. För övriga inställningar, se hjälpfunktionen ?write_sav.\n\n\n9.2.2 CSV\nDataformatet vi föredrar är kommaseparerade värden, eller csv (eng. comma-separated values). Detta är ett maskin- och människoläsbart format, där varje datarad är en… rad och värden för raderna listas upp separerade med kommatering - därav namnet. Ungefär så här ser en CSV-fil ut:\n1,1,1,1,5,2\n1,1,1,2,3,7\n0,0,2,2,3,2\n1,1,,2,6,3\n,,,,,\n0,0,0,0,0,0\nFör att förenkla läsningen av CSV-filer kan du öppna dem i ett kalkylbladprogram så som Excel eller Calc, och se varje värde uppställt i separata celler. Det är dock bra att förstå, att under huven är alla värden bara listade på ovan sätt, lite som en väldigt stor textfil.\nTanken bakom CSV är att formatet ska vara ett öppet, lätt användbart och enkelt strukturerat datasystem. Det är därför lite synd, att det finns olika former av CSV - och du måste bara veta vilken form du har.\nFormat 1 använder komma som separator (därav namnet CSV) och punkt som decimalskiljetecken: 1.5,2.7 är två värden, det andra större än det första. Format 2 använder semikolon som separator och komma som decimalskiljetecken: 1,5;2,7 för samma värde i format 2. Det andra används bl.a. i Norden, eftersom vi tenderar att skriva decimaltal med komma som skiljetecken.\nDu kan läsa in CSV-filer med två funktioner från Tidyverse-paketet readr: read_csv(path) och read_csv2(path). Bägge tar som obligatoriska argument filadressen och returnerar en tibble av de avlästa data. Den första använder kommaseparator, den andra semikolonseparator. Glömmer du vilken är vilken kan du alltid kolla upp ?read_csv2.2 Märk, att det finns liknande funktioner i paketet foreign om du har det aktiverat. Var noggrann med vad du skriver och hur!\n\n\nreadr::read_csv(path): Läser in CSV-data från filen path. Använder kommaseparator och punktdecimal. Returnerar en tibble. readr::read_csv2(path): Läser in CSV-data från filen path. Använder semikolonseparator och kommadecimal. Returnerar en tibble.\nFör att skriva filer använder du motsvarande funktioner write_csv(data, path) (kommaseparation) och write_csv2(data,path) (semikolonseparation) från readr. Första argumentet ger data du vill skriva till en fil, andra argumentet namnet på filen. Filen sparas i din arbetsfilmiljö om du inte specificerar en noggrannare filadress.\n\n\nreadr::write_csv(x, path): Skriver objektet x till csv-filen vid namnet och adressen path. Använder kommaseparation och punktdecimal. readr::write_csv2(x, path): Skriver objektet x till csv-filen vid namnet och adressen path. Använder semikolonseparation och kommadecimal.\n\n\n9.2.3 Andra format\nOm du får data i andra format kan du först skrika inombords - varför måste det vara så svårt?! - och sedan lugna dig själv med tilltron i R-samhörigheten. Högst troligen finns det en funktion som kan läsa in de mystiska data du har fått, men du måste lite rota. Paketen readr, foreign och haven innehåller många olika läs- och skrivfunktioner. Börja med dem, och om du inte hittar en passande funktion, gå sedan till Google för hjälp. Om inget hjälper, säg åt dataägaren att de tar sig i kragen och levererar ett dataformat som vanliga dödliga kan avläsa.\n\n\n9.2.4 Övning: Läs in barometerdata till R\nNu övar vi datainläsning! Vi hämtar in data från Finlandssvenska barometern 2008 och sparar det som ett tibble-objekt med namnet data. Jag gör koden aningen mer explicit än vad som behövs, för pedagogiska syften.\n\nlibrary(readr) # Aktivera readr. Om paketet är aktivt gör denna rad ingenting.\npath <- \"FSD3200/Study/Data/daF3200.csv\" # Byt detta till adressen där du har datafilen.\ndata <- read_csv2(path)\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 819 Columns: 116\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr   (1): fsd_li\ndbl (115): fsd_no, fsd_vr, fsd_id, q1a_lk, q2_lk, q3, q4, q5, q6, q7, q8, q9...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNu borde våra data finnas inladdade i objektet data! Vi kan granska detta genom att kalla data, vilket borde ge en förkortad version av tibbleobjektet:\n\ndata\n\n# A tibble: 819 × 116\n   fsd_no fsd_vr fsd_id fsd_li  q1a_lk q2_lk    q3    q4    q5    q6    q7    q8\n    <dbl>  <dbl>  <dbl> <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1   3200      1      1 (A) öp…      2     6     1     7     4     3     2     0\n 2   3200      1      2 (A) öp…      2     5     1     6     4     3     2    NA\n 3   3200      1      3 (A) öp…      2     6     1     4     4     1     1    NA\n 4   3200      1      4 (A) öp…     NA     6     2     3     4     3     2    NA\n 5   3200      1      5 (A) öp…      4     6     1     3     4     3     2    NA\n 6   3200      1      6 (A) öp…      9     5     2     7     4     3     2    NA\n 7   3200      1      7 (A) öp…      2     6     1     3     4     1     1    NA\n 8   3200      1      8 (A) öp…      2     6     1     3     4     3     2    NA\n 9   3200      1      9 (A) öp…      9     6     2     7     4     1     1    NA\n10   3200      1     10 (A) öp…      2     2     1     2     1     3     2    NA\n# … with 809 more rows, and 104 more variables: q9_1 <dbl>, q9_2 <dbl>,\n#   q9_3 <dbl>, q9_4 <dbl>, q9_5 <dbl>, q9_6 <dbl>, q10 <dbl>, q11_1 <dbl>,\n#   q11_2 <dbl>, q11_3 <dbl>, q11_4 <dbl>, q11_5 <dbl>, q12_1 <dbl>,\n#   q12_2 <dbl>, q12_2b <dbl>, q12_2c <dbl>, q12_3 <dbl>, q12_4 <dbl>,\n#   q12_5 <dbl>, q12_6 <dbl>, q12_7 <dbl>, q12_8 <dbl>, q12_9 <dbl>,\n#   q12_10 <dbl>, q12_11 <dbl>, q12_12 <dbl>, q13_1 <dbl>, q13_2 <dbl>,\n#   q14_1 <dbl>, q14_2 <dbl>, q15_1 <dbl>, q15_2 <dbl>, q15_3 <dbl>, …\n\n\nAllt fungerade bra! Från förhandsgranskningen av tibbleobjektet ser vi att vi har 819 rader data (respondenter) och 116 kolumner (statistiska variabler). Inläsningsfunktionen producerar även en hel del debuggningstext, som kan vara nyttigt om du märker fel i inläsningen. För att försäkra oss att allt gick korrekt kan vi öppna den ursprungliga csv-filen i ett kalkylbladprogram och jämföra visuellt med vårt inlästa objekt. Gör du detta, så märker du att saknande värden (tomma celler i CSV-filen) har konverterats till NA-värden i R - något vi oftast vill att ska ske, som tur.\nOm du jobbar i RStudio kan du även öppna data-objektet från Environment-tabben i övre högra hörnet. I denna vy kan du visuellt bearbeta materialet, t.ex. lägga på olika värdefilter eller t.o.m. ändra enskilda datapunkters värden. Vi strävar däremot att jobba i kod så långt som möjligt för pedagogiska syften - det är bäst att du först lär dig kod, sedan lättare verktyg, så att du kan lösa mer komplexa problem i datahanteringen om/när de uppstår. Sträva alltså till att göra saker i kod först, och ta de visuella verktygen i bruk senare.\nDäremot har vi ett problem: vi vet ju inte vad dessa variabler betyder! I nästa kapitel hanterar vi några grundläggande datamanipuleringsmetoder i R, och putsar upp vårt dataset till något mer brukbart."
  },
  {
    "objectID": "grafik.html#varför-grafik",
    "href": "grafik.html#varför-grafik",
    "title": "11  Grafik",
    "section": "11.1 Varför grafik?",
    "text": "11.1 Varför grafik?\nI grund och botten finns grafiska uppställningar till för att lösa två problem:\n\nHur summerar jag stora mängder siffror på ett översiktligt sätt?\nHur uttrycker jag svårförståeliga fenomen på ett enkelt sätt?\n\nMeningen med vetenskaplig grafik är alltså att undvika massiva tabeller med hundratals celler, och att förenkla svåra fenomen med lättbegripliga visuella hjälpmedel.\nDet är värt att poängtera några ytterligare regler, som ofta glöms bort när forskaren skapar fina bilder och komplexa grafer:\n\nOm grafen inte förenklar något svårt, så är den inte av nytta.\nOm grafen inte används i analysen, så är den inte av nytta.\nOm grafen upprepar något som lätt kan förstås med tabeller eller text, så är den inte av nytta.\n\nDen tredje punkten kanske upprepar den första, men det är värt att upprepa: grafer ska förenkla förståelse, inte göra texten estetiskt vackrare! Använd alltså grafer med fri hand, men kom ihåg att de faktiskt måste gynna textförståelsen.\nDen andra punkten är viktig, eftersom detta glöms ofta bort. Det sägs att en bild säger mer än tusen ord, men jag tycker att ordspråket bör modifieras för vetenskapligt bruk: en bild säger ingenting om du inte förklarar den. Du bör alltid faktiskt använda grafen i din text! Du kan tänka dig meningar så som:\n\nDetta [tidigare förklarat] förhållande syns tydligt i Figur 1 nedan.\nSe Figur 2: variabelns felmätning (y-axeln) uppvisar tydlig variansheterogenitet.\nResultaten för regressionsmodellen är summerade i Figur 3 nedan, där varje sektion visar det kontrollerade, standardiserade förhållandet mellan beroende och oberoende variabel."
  },
  {
    "objectID": "grafik.html#att-göra-grafik-ggplot2",
    "href": "grafik.html#att-göra-grafik-ggplot2",
    "title": "11  Grafik",
    "section": "11.2 Att göra grafik: ´ggplot2´",
    "text": "11.2 Att göra grafik: ´ggplot2´\nI R finns det i princip två stora sätt att producera grafik: de inbyggda grafiska verktygen och paketet ggplot21. Vi kommer att lära oss det sedan nämnda, eftersom det är mer flexibelt. R:s inbyggda paket kan dock vara behändigt om du behöver snabbt granska ett förhållande (t.ex. lådgrafer för flera variabler), så tänk inte att du måste använda ggplot. Som alltid finns det flera sätt att lösa ett problem, och det bästa sättet är det du kan.1 Siffran 2 står för version två. För att förenkla talar jag om ggplot i fortsättningen av texten, men syftar till den nyaste versionen vid skrivande stund.\n\n11.2.1 Grafikens grammatik\nggplot följer något som kallas grafikens grammatik (eng. grammar of graphics). Detta är en designfilosofi som förenklar jämförelsen av olika grafiker och gör det lättare för dig att lära dig en ny grafisk funktion. Alla grafer i ggplot följer samma princip:\n\nggplot(data = mitt.data,        # Obligatorisk: Skapar grafen.\n       aes(x = var1, y = var2,\n           ...)\n       )\n   + geom.funktion()            # Obligatorisk: Ritar data.\n   + coordinate.funktion()      # Valfri: Koordinatsystemets inställningar.\n   + facet.funktion()           # Valfri: Delar grafen i subgrafer.\n   + scale.funktion()           # Valfri: Ställer in grafens skala.\n   + theme.funktion()           # Valfri: Ändrar grafens visuella tema.\n\nDu inleder en graf genom funktionen ggplot(). Beroende på vad du vill göra kallar du funktionen på olika sätt:\n\nggplot() skapar en helt tom graf. Du kan fylla den med olika element från olika dataset helt fritt. Använd när du använder många olika dataset och många olika grafiska element.\nggplot(data) skapar en tom graf med ett färdigt dataset. Du kan lätt kalla olika kolumner från ditt dataset genom implicit hänvisning - t.ex. aes(x = var1) istället för aes(x = data$var1). Använd när du använder främst ett dataset men många olika grafiska element.\nggplot(data, aes()) skapar en graf med de grafiska inställningarna givna i argumenten för aes(). Använd när du använder främst ett dataset och främst en grafisk uppställning.\n\nSyftet med olika sätt att generera grafen är alltså att ge dig den nivå av flexibilitet du behöver. I alla situationer kan du lägga till grafiska element fritt, men det är helt enkelt lättare ibland att börja med en tom graf. Se nedan för några exempelgrafer.\n\ndatabas <- tibble(a = 1:1000, b = rnorm(1000))\n\nggplot() + ggtitle(\"Tom graf: ggplot()\")\nggplot(data = databas) + ggtitle(\"Tom graf med färdigt data: ggplot(data)\")\nggplot(data = databas, aes(x = a, y = b)) + ggtitle(\"Graf med data: ggplot(data, aes())\")\n\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Geom-element\nSom du säkert märkte ritade funktionerna inget data i graferna. Varför det, må du undra? Det är ganska enkelt: vi har berättat åt ggplot följande saker: “Hej ggplot, skapa en graf! Hej ggplot, skapa en graf med data från databas! Hej ggplot, skapa en graf med data från databas, och sätt kolumnen a på x-axeln och kolumnen b på y-axeln!\nMen märk: ingenstans sade vi åt ggplot hur den ska rita upp vårt data! Grafiska element kallas geoms i ggplot-språket, och för att se ett grafiskt element, måste vi förstås skapa en geom. Följande grafer ger exempel på sådana geoms:\n\nggplot(data = databas) + geom_area(aes(y = a, x = b)) + ggtitle(\"geom_area()\")\nggplot(data = databas) + geom_density(aes(x = b)) + ggtitle(\"geom_density()\")\nggplot(data = databas) + geom_histogram(aes(x = a)) + ggtitle(\"geom_histogram()\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nNågra saker att poängtera: * Jag har flyttat aes() till geom-funktionerna istället för basfunktionen. Om du skulle lägga till flera geoms med denna metod, så måste du definiera estetiken för varje geom skiljt - eftersom det inte finns en estetik i basfunktionen som de andra funktionerna kan ärva. * De flesta grafer ser urusla ut, förutom geom_density(). Det finns många olika grafiska metoder, och de passar olika data olika bra.\nVi kan fortsätta med geom_density(). Grafen ser väldigt lika en normalfördelning, kanske vi vill exemplifiera detta genom att lägga en normalfördelningskurva ovanpå grafens data? Vi kan göra detta lätt, genom att lägga ett nytt element på grafen: geomen geom_function().\n\n# Först återskapar jag den tidigare grafen i ett objekt:\ngraf <- ggplot(data = databas) + geom_density(aes(x = b))\n\n# Sedan lägger jag till ett nytt element:\ngraf + geom_function(fun = dnorm,\n                     args = list(sd = 1),\n                     aes(x = b, colour = \"red\"),\n                     show.legend = FALSE)\n\n\n\n\nFunktionen tar många olika argument, så vi går igenom dem steg för steg.\nFörsta argumentet är fun, alltså funktion2, och bestämmer vilken funktion som ska ritas. Jag bad R rita funktionen dnorm(), vilket genererar en densitetskurva av normalfördelningen. Detta är samma kurva som tidigare, men nu skapar den en perfekt normalfördelning.2 Funktioner är roliga, enligt R och mig.\nAndra argumentet är args, alltså argument. Detta kan vara lite svårtydigt: här listar jag argumenten jag vill ge åt funktionen som jag bestämde tidigare. Istället för att skriva dnorm(sd = 1) ger jag alltså funktionen som ett argument, fun = dnorm och funtionens argument som ett annat argument args = list(sd = 1). Argumenten måste ges som en lista, även om jag bara ger ett argument sd = 1.\nTredje argumentet är aes(), alltså estetiken. Här definierar jag först att jag vill ha x-axeln rita upp kolumnen b från datasetet dataset, i enlighet med vad vi definierade tidigare. Varför måste jag säga detta på nytt? Jo, eftersom estetiken inte har definierats i basfunktionen ggplot! Grafiska element ärver estetiken från deras basfunktioner, så om basfunktionen inte har en estetik, så får inte elementet heller automatiskt en estetik. Detta betyder också att vi kunde ge denna linje en annan estetik, om vi ville. Du kan testa vad detta gör i din egen kod.\nInom linjens estetik berättar jag också att jag vill ha en röd linje, med argumentet colour = \"red\"3. Vi kan också ge andra estetiker än färg: se hjälpfunktionen vignette(\"ggplot2-specs\") för information på alla estetiska argument - det finns många!3 ´ggplot´ är intelligent och kan skilja mellan engelsk och amerikansk stavning: color, col, colour ger alla samma resultat.\nDet sista argumentet är show.legend = FALSE, och säger åt ggplot att inte rita en förklaringsruta. Du kan prova lämna bort detta för att se vad den ritar - rutan är inte nödvändig i denna graf.\nVi kan fortsätta stapla på geoms, hur många som helst! I något skede blir dock grafen obegriplig, så var sparsam. Skapa hellre flera grafer än att du tvingar en massa vetenskapliga argument in i en graf.\n\n\n11.2.3 Text på grafen\nOfta vill vi lägga till text på grafen. Detta kan vara en rubrik, underrubrik, alternativ text, text på koordinataxlarna, små textrutor här och där, och så vidare. ggplot sköter saken med ett antal olika funktioner. Dessa funktioner läggs till på samma sätt som alla andra element, genom att använda +-operatorn på baselementet.\nFunktionen labs() ställer in de flesta standardtexter:\n\ngraf + labs(x = \"X-axelns rubrik\",\n            y = \"Y-axelns rubrik\",\n            tag = \"Tag-texten\",\n            title = \"Grafens rubrik\",\n            subtitle = \"Grafens underrubrik\",\n            caption = \"Rubrik under grafen\",\n            alt =\"Alternativ text\")\n\n\n\n\nArgumenten torde vara självförklarliga. Ett undantag är alt, eller alternativa texten. Denna text syns i vissa specialfall, så som om grafen inte lyckas genereras (om du t.ex. skriver en HTML-bok, så som jag gör) eller om läsaren använder en skärmläsare. Du kan använda funktionen get_alt_text() för att hämta alternativa texten från en graf. Det är god form att skriva alternativ text för att göra dina grafer tillgängliga.\nEtt problem med standardtexterna är att du har väldigt lite kontroll över hur de ser ut och var de placeras. Om du vill göra mer komplexa textmarkeringar finns det ett annat verktyg, annotate():\n\ngraf + annotate(geom = \"text\",\n                x = 2,\n                y = 0.3,\n                colour = \"blue\",\n                family = \"serif\",\n                fontface = \"bold\",\n                label = \"Min text\")\n\n\n\n\nFunktionen tar tre(ish) obligatoriska argument och en rad valfria argument.\nDet första obligatoriska argumentet är geom, som bestämmer hurudan geom du vill lägga på grafen. I textfall vill du använda “text”, men du kunde lägga in datapunkter, linjer, flerhörningar, bilder, och så vidare.\nDet andra obligatoriska argumentet är, förstås, koordinaterna. Hur många koordinater är obligatoriskt beror på vilken geom du ritar. För text måste du definiera en x-koordinat och en y-koordinat, men om du t.ex. skulle rita en fyrhörning (med geom = \"rect\"), skulle du måste definiera dess fyra hörn.\nDet sista obligatoriska argumentet är själva texten, label. Du måste ju skriva något på grafen för att skriva på grafen! Andra geoms än “text” har andra krav.\nYtterligare argument kan ges till annotate - dessa är estetiska argument som du vanligen skulle ge åt aes(). Jag använde colour för att göra texten blå, family för att byta fonten till en serif-font, samt fontface för att göra texten svärtad.\nEtt bra trick att veta är att du kan också föra in resultat från datafunktioner inom texten. Detta gör du genom att ge label-argumentet funktionen paste(), vilken slår ihop R-objekt med vanlig text:\n\ngraf + annotate(geom = \"text\",\n                x = -2,\n                y = 0.3,\n                colour = \"red\",\n                label = paste('N =', count(databas)))\n\n\n\n\nFunktionen annotate() är flexibel och kan lägga in vadsomhelst för former på grafen. För enbart text kan du även använda två enklare funktioner: geom_text() och geom_label(), så som nedan:\n\ngraf + geom_text(x = -2,\n                 y = 0.25,\n                 label = \"geom_text()\") +\n  geom_label(x = -2,\n             y = 0.2,\n             label = \"geom_label()\")\n\n\n\n\nSom vanligt bör du ge x- och y-koordinater åt funktionerna, samt själva texten i argumentet label. Utöver dessa kan du ge ett stort antal olika estetiska argument, varav några viktiga är nedan:\n\ncolour bestämmer färgen.\nfamily bestämmer fonten (sans, serif eller mono).\nfontface bestämmer fonteffekten, t.ex. svärtning.\ncheck_overlap tar ett sanningsvärde och bestämmer huruvida texter får ligga på varandra inom grafen.\nvjust bestämmer justeringen av texten\n\nSe hjälpfunktionen ?geom_text eller ?geom_label för mera information om alternativen."
  },
  {
    "objectID": "grafik.html#många-grafer-samtidigt",
    "href": "grafik.html#många-grafer-samtidigt",
    "title": "11  Grafik",
    "section": "11.3 Många grafer samtidigt",
    "text": "11.3 Många grafer samtidigt"
  },
  {
    "objectID": "univariat-analys.html#lägesmått",
    "href": "univariat-analys.html#lägesmått",
    "title": "11  Univariat analys",
    "section": "11.1 Lägesmått",
    "text": "11.1 Lägesmått\n\n11.1.1 Medeltal\nDet första av lägesmåtten är medeltalet. Definitionen på (aritmetiska) medeltalet är tämligen enkel:\n\\[ \\bar{x} = \\frac{x_i + x_{i+1} + ... + x_{n-1} + x_n}{n} = \\frac{\\sum_{i=1}^{n}{x_i}}{n} \\] I klarspråk: Medeltalet är lika med summan av alla individuella värden från \\(1\\) till \\(n\\), dividerat med antalet värden \\(n\\).\nMedeltalet ger dig en siffra som berättar kring vilket värde variablens fördelning är centrerad. Däremot berättar den inget om spridningen runt detta värde. Följande tre beräkningar är tämligen olika men ger samma medeltal:\n\n\\((1 + 2 + 3)/3 = 2\\)\n\\((1 + 4 + 1)/3 = 2\\)\n\\((0 + 0 + 6)/3 = 2\\)\n\nMedeltalet är också sensitivt för extrema värden. Dessa är värden som ligger långt från de andra. Se följande exempel:\n\\[ \\bar{x} = \\frac{1 + 2 + 1 + 4 + 300}{5} = 61.6 \\] Medeltalet beskriver förvisso centret av fördelningen, men det är ju mycket högre än de flesta datapunkter! Som tur finns det verktyg för att hantera detta, som vi behandlar inom kort.\nMedelvärde kräver att variabeln är på kvantitativ skalenivå, alltså intervallskala eller kvotskala. Ett medelvärde kan alltså inte räknas på nominella eller ordinella variabler, eftersom det blir meningslöst. Tänk dig att du har samlat data på personer och deras hemkommuner, delat i tre nivåer: Landsbygdskommun, tätortskommun och urban kommun. Du kanske har gett dessa tre kategorier värden, så som 1, 2 och 3, eftersom det är lättare att hantera siffror än text. Det vore dock absurt att säga att “Medelpersonen hade kommuntyp 1.54” eller “Den vanligaste kommuntypen var 2.3”. Vad betyder det? Landsortskommun? Täturban kommun? Landorban kommun?\nI R kan du beräkna ett medeltal med funktionen mean(data):\n\nmean(data)\n\n[1] 0.04619816\n\n\n\n\n11.1.2 Median\nDet andra lägesmåttet är medianen. Dess matematiska definition är onödigt komplex, då det är lättare att förstå medianen med ord. Medianen är värdet som skiljer den största halvan från den minsta halvan av datafördelningen. Låt oss säga att vi har följande datafördelning:\n\\[ D = \\{4, 3, 8, 19, 2, 6, 2, 57, 2\\} \\] Vi kan hämta medianen genom att rangordna alla värden från lägst till högst (eller vice versa):\n\\[ D = \\{2, 2, 3, 4, 6, 8, 19, 57\\} \\] Medianen är värdet som ligger i mitten: \\(6\\). Om antalet värden är jämnt är medianen medeltalet av de två mittersta värden.\nMedianen kan användas på ordinal- eller kvantitativ skala, alltså alla skalenivåer förutom nominalskala.\nI R kan du hämta medianen med funktionen median(data):\n\nmedian(data)\n\n[1] 0.04621674\n\n\n\n\n11.1.3 Typvärde\nDet tredje lägesmåttet är typvärde. Det kan även definieras enkelt med ord: det är det oftast förekommande värdet i datafördelningen. I den föregående fördelningen är typvärdet \\(2\\), eftersom det fanns två förekomster av \\(2\\) jämfört med en förekomst av alla andra värden.\nTypvärde kan användas på alla skalenivåer.\nR har, förvånansvärt nog, inte en inbyggd funktion för typvärde. För nominal- eller ordinalskalevariabler kan du använda funktionen table(data) för att finna typvärdet:\n\nordinal <- c(2, 3, 3, 2, 3, 1, 3, 3, 2, 4, 1, 2, 3, 0)\ntable(ordinal)\n\nordinal\n0 1 2 3 4 \n1 2 4 6 1 \n\n\nTabellens första rad visar variabelvärdet, andra raden visar frekvensen. Typvärdet för variabeln ordinal är \\(3\\), som förekommer 6 gånger i materialet.\n\n\n11.1.4 Kvartiler och percentiler\nDe fjärde och femte lägesmåtten är kvartiler och percentiler. Dessa är något besläktade till medianen. Om du skulle ordna alla tal i din datafördelning från lägst till högst, så representerar kvartilerna gränsen för 25 och 75 procent av data (50%-gränsen är medianen), och percentilerna representerar var tionde procentgräns. Med andra ord: den lägre kvartilen är värdet som högst 25 procent av ditt data underskrider, den högre kvartilen är värdet som minst 75 procent av ditt data underskrider, och percentilerna är tio-procents-kategorier från 10 till 90 procent.\nR använder samma verktyg för att hämta kvartiler och percentiler, eftersom de konceptuellt är samma sak (men med olika procentgränser), quantile(data, probs = x)\n\nkvartiler <- quantile(data, probs = seq(0, 1, 0.25))\nround(kvartiler, 2)\n\n   0%   25%   50%   75%  100% \n-2.78 -0.60  0.05  0.69  3.33 \n\npercentiler <- quantile(data, probs = seq(0, 1, 0.1))\nround(percentiler, 2)\n\n   0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n-2.78 -1.24 -0.77 -0.45 -0.20  0.05  0.31  0.58  0.85  1.28  3.33 \n\n\nFrån resultaten kan vi bl.a. se att 10% av data ligger på \\(-1.24\\) eller lägre, 25% av data ligger på \\(-0.60\\) eller lägre, medianen är \\(0.05\\), och så vidare. Funktionen ger mer exakta värden än två decimaler, men för läsbarhetens skull är det lättare att avrunda något."
  },
  {
    "objectID": "univariat-analys.html#grafisk-uppställning-av-lägesmått-boxplot",
    "href": "univariat-analys.html#grafisk-uppställning-av-lägesmått-boxplot",
    "title": "11  Univariat analys",
    "section": "11.2 Grafisk uppställning av lägesmått: boxplot()",
    "text": "11.2 Grafisk uppställning av lägesmått: boxplot()\nFöre vi går vidare till spridningsmått är det värt att nämna ett mycket behändigt verktyg. Eftersom det finns flera lägesmått, har några smarta statistiska grafiker kommit på ett verktyg för att summera de flesta i en och samma graf: lådgrafen (eng. box plot).\nLättare visat än beskrivet. Du kan generera en lådgraf med funktionen boxplot(data):\n\ngraf <- boxplot(data)\n\n\n\n\nLådgrafen visar ett antal olika statistiker samtidigt: den tjocka svarta linjen i mitten är medianen, och lådans övre och nedre gränser är kvartilgränserna (25 och 75 procent). De horisontella linjerna långt ute är gränserna för vad som kallas extrema värden. De definieras i förhållande till det interkvartila avståndet. För exakta siffror kan du hämta $stats på den resulterande lådgrafen:\n\ngraf$stats\n\n            [,1]\n[1,] -2.46693865\n[2,] -0.59724817\n[3,]  0.04621674\n[4,]  0.68943528\n[5,]  2.49691851\n\n\nSiffrorna står, i ordning, för: lägre extremvärdesgränsen, lägre kvartilen, medianen, högre kvartilen, och högre extremvärdesgränsen."
  },
  {
    "objectID": "univariat-analys.html#spridningsmått",
    "href": "univariat-analys.html#spridningsmått",
    "title": "11  Univariat analys",
    "section": "11.3 Spridningsmått",
    "text": "11.3 Spridningsmått\nHittills har vi diskuterat hur du beskriver din datafördelning med en siffra. Vi har även använt flera olika siffror för att börja förstå fördelningen i en större helhet. Vi kan dock förenkla fördelningens beskrivning till ytterligare några nyckeltal: spridningsmåtten. Dessa beskriver hur data sprider sig runt medeltalet i medeltal.\n\n11.3.1 Percentilavstånd\nDet första spridningsmåttet är percentilavstånd (eng. percentile range). Detta används jämförelsevis sällan, men kan ge dig nyttig information. Percentilavståndet står helt enkelt för avståndet mellan två percentiler. Oftast syftar percentilavståndet dock på avståndet mellan 10:e percentilen och 90:e percentilen, alltså mellan 10 och 90 procent av data.\nDu beräknar percentilavståndet för hand väldigt enkelt: \\(p_{90}-p_{10}\\). Resultatet är mätt i samma skala som ditt data. Percentilavstånd kan beräknas på ordinal- samt kvantitativa skalenivåer, eftersom percentiler kan beräknas på dem.\nPercentilavstånd kan inte direkt räknas med basfunktionerna i R, men ärligt talat: du fixar det för hand med ekvationen ovan.\n\n\n11.3.2 Varians\nDet andra spridningsmåttet heter varians. Variansen kan beräknas på kvantitativa skalenivåer, eftersom den relaterar till medeltalet. För att förenkla variansen bör vi först definiera ett antal termer.\nVi börjar med avvikelse. Detta innebär avståndet mellan datafördelningens medeltal och en enskild datapunkt:\n\\[\\text{avvikelse} = x-\\bar{x}\\] Detta kvantifierar hur mycket “fel” medeltalet förutspår den enskilda respondentens svar. Ju större, desto längre ifrån det rätta svaret.\nSummerar vi ihop avvikelserna för varje datapunkt får vi den totala mängden avvikelse, med ett problem: summan blir alltid noll! Detta sker eftersom medeltalet alltid landar någonstans emellan det största och det minsta värdet, alltså finns det både positiva och negativa värden. Inte bara “någonstans”, faktiskt, utan exakt i mitten - det är ju definitionen på medeltalet.\n\\[\\text{S(avvikelse)} = \\sum_{i=1}^{n}x_i - \\bar{x} = 0 \\] Vi kan dock fixa detta lätt: vi kvadrerar varje enskild avvikelse, alltså multiplicerar den med sig själv. Eftersom två negativa multipliceras ihop till en positiv, måste alla resulterande värden vara positiva. Sedan summerar vi ihop alla kvadratvärden, för att få avvikelsens kvadratsumma SSE (från eng. sum of squared error).\n\\[\\text{SSE} = \\sum_{i=1}^{n}(x_i-\\bar{x})^2\\] Vi är nästan där! Siffran vi har just nu representerar den totala mängden kvadratfel, alltså hur mycket total felmätning medeltalet orsakar, men i kvadrat. Det finns några problem kvar att lösa:\n\nDenna siffra är beroende på urvalsstorleken: ju fler datapunkter, desto större siffra, eftersom varje enskild datapunkt adderas till summan.\nMen vadå “kvadratfel”? Obegripligt!\n\nProblem ett löser vi nu, problem två löser vi strax. Genom att dividera kvadratsumman med \\(N-1\\) får vi variansen - ett mått på hur mycket kvadratfel det i medeltal förekommer i datafördelningen. Orsaken till varför vi inte dividerar med N (så som man skulle göra med ett vanligt medeltal) är något komplex. Dividenden kallas för frihetsgraden (eng. degrees of freedom, df), och kan variera något beroende på vilken statistisk beräkning som görs. Eftersom R sköter beräkningen av dessa för dig i de allra flesta fallen, behöver du inte bry dig om hur siffran har kommit till1.1 Läs gärna vidare vid KÄLLA om du trots allt vill veta vad frihetsgraden betyder.\nDärmed har vi variansen, \\(s^2\\):\n\\[s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{N-1} \\] För att göra ekvationen tydligare har jag raderat summafunktionens gränser, men de finns ännu där!\nI R kan du beräkna variansen med funktionen var(data):\n\nvar(data)\n\n[1] 0.9974968\n\n\nKom ihåg att variansen mäts som kvadraten av din ursprungliga skala; om skalan är centimetrar är variansen i kvadratcentimeter, om skalan är celsius är den kvadratcelsius, och om skalan är euro är den… kvadrateuro.\n\n\n11.3.3 Standardavvikelse\nDet sista spridningsmåttet fixar problemet med variansen: standardavvikelsen. Vi gjorde redan nästan alla beräkningar, och den sista står framför oss: vi måste ta kvadratroten på variansen.\n\\[\\text{SD} = \\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{N-1}} \\] I R räknar du standardavvikelsen med funktionen sd(data):\n\nsd(data)\n\n[1] 0.9987476\n\n\nStandardavvikelsen representerar hur mycket felmätning det finns mellan medeltalet och den enskilde datapunkten i medeltal. En större standardavvikelse innebär mera spridning runtom medeltalet, eftersom datapunkterna i medeltal ligger längre borta från medeltalet. En mindre standardavvikelse innebär mindre spridning. Graferna nedan exemplifierar detta:\n\nlibrary(ggplot2)\ngrafbas <- ggplot() + xlim(-3, 3)\ngrafbas + geom_function(fun = dnorm, args = list(sd = 0.1)) + ggtitle(\"SD = 0.1\")\ngrafbas + geom_function(fun = dnorm, args = list(sd = 0.5)) + ggtitle(\"SD = 0.5\")\ngrafbas + geom_function(fun = dnorm, args = list(sd = 1)) + ggtitle(\"SD = 1\")\ngrafbas + geom_function(fun = dnorm, args = list(sd = 2)) + ggtitle(\"SD = 2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nVid \\(SD = 0.1\\) är datafördelningen väldigt stram runt medeltalet \\(\\bar{x}=0\\)2, men när standardavvikelsen ökar, sprider sig fördelningen mera ut runtom medeltalet.2 Eller ungefär så: medeltalet är inte nödvändigtvis perfekt noll, eftersom jag har genererat grafen med en urvalsfunktion. Kan du tänka dig varför?\n\n\nVarför får jag inte fyra fina grafer?\n\nOm du provade köra ovan kodsnutt för dig själv, märkte du snabbt att den inte producerade en fin figur med fyra separata grafer. Istället producerade den bara den sista grafen! Orsaken är, att grafproduktionen för denna bok specifikt fungerar något annorlunda än grafproduktionen i RStudio. Bakom scenen har jag berättat åt R att den ska generera fyra grafer i två rader och två kolumner, men om du kör koden, förs inte denna information vidare.\nVill du göra samma behöver du en funktion från ett ytterligare paket, gridExtra. Funktionen heter: grid.arrange(graf_1, graf_2, ..., graf_n, ncol = i, nrow = j). Du ger funktionen först alla grafer du vill ordna, i ordning (så generera dem först och spara dem i objekt), och sedan berättar du hur många kolumner ncol och hur många rader nrow du vill fördela graferna över. Det behöver inte finnas tillräckligt med grafer för att fylla rutan."
  },
  {
    "objectID": "univariat-analys.html#osäkerhetsmått",
    "href": "univariat-analys.html#osäkerhetsmått",
    "title": "11  Univariat analys",
    "section": "11.4 Osäkerhetsmått",
    "text": "11.4 Osäkerhetsmått\n\n11.4.1 Standardfel\n\n\n11.4.2 p-värde\n\n\n11.4.3 Konfidensintervall"
  },
  {
    "objectID": "univariat-analys.html#grafisk-uppställning",
    "href": "univariat-analys.html#grafisk-uppställning",
    "title": "11  Univariat analys",
    "section": "11.5 Grafisk uppställning",
    "text": "11.5 Grafisk uppställning"
  },
  {
    "objectID": "bivariat-analys.html#korstabulering",
    "href": "bivariat-analys.html#korstabulering",
    "title": "12  Bivariat analys",
    "section": "12.1 Korstabulering",
    "text": "12.1 Korstabulering\nDu har tidigare blivit bekant med frekvenstabeller i denna bok. En frekvenstabell, för att påminna, är en tabell som innehåller information på hur många analysobjekt har fått ett visst värde på variabeln i fråga. Frekvenstabeller är därmed univariata metoder, eftersom de presenterar information på en variabel.\nKorstabellen (eng. crosstabulation eller förkortat crosstab) är en bivariat version av detta. Istället för att lägga variabelnivåerna i raderna och frekvensen i kolumnerna, lägger vi den beroende variabeln i raderna och den oberoende variabeln i kolumnerna1.1 Egentligen är ordningen arbiträr, du kan likväl ha beroende i kolumnerna och oberoende i raderna. Av konvention brukar man följa ordningen jag nämner, men gör hur du vill - så länge du säger tydligt vad du menar.\nIdén med korstabellen är att vi kan granska enskilda frekvenser för vissa kategorier, och därmed se eventuella skillnader i beroende variabelns grad enligt oberoende variabel. Ser vi skillnader så tyder det på att det finns ett förhållande mellan beroende och oberoende variablerna. Om vi inte ser någon skillnad, så finns det troligen inte ett förhållande (men glöm inte elaborering!).\nI R kan du utföra en korstabulering med funktionen stats::xtabs(formula, data). Funktionen tar två obligatoriska argument: formula beskriver vad som ska korstabuleras, och data beskriver varifrån data ska hämtas. Funktionen returnerar en korstabell:\n\nkorstabell <- xtabs(~alcgp + tobgp, data = esoph)\nkorstabell\n\n           tobgp\nalcgp       0-9g/day 10-19 20-29 30+\n  0-39g/day        6     6     5   6\n  40-79            6     6     6   5\n  80-119           6     6     4   5\n  120+             6     6     5   4\n\n\n[BYT TILL BÄTTRE EXEMPEL]\nVi kan se en tabell med två variabler: alcgp i raderna, vilket står för gram av alkohol per dag, och tobgp, vilket står för gram av tobak per dag. Om vi granskar cellerna så tycks det inte finnas ett förhållande mellan alkoholkonsumption och tobakskonsumption i vårt data: alla celler har ungefär lika stora värden i sig.\n\n12.1.1 Chi-kvadrat-testet: \\(\\chi^2\\)\nChi2-kvadrat-testet är ett osäkerhetstest för korstabeller. Vi kan använda chi-kvadrat-testet för att undersöka huruvida våra observerade cellvärden (vårt data) motsvarar vad vi skulle förvänta oss under nollhypotesen. I detta fall är nollhypotesen att det inte finns ett förhållande mellan de två variablerna i korstabellen.2 Uttalas [\\(k^h\\)i:], med hårt K-ljud\n\\(\\chi^2\\)-testet kan förstås som en generell lineär modell (GLM). Detta betyder att den gör samma sak som alla andra modeller: \\(\\text{Utsaga} = \\text{Modell} + \\text{Felvärde}\\). De facto räknar \\(\\chi^2\\) faktiskt avvikelsens kvadratsumma (se kapitel XXX), men med en liten modifikation:\n\\[\\chi^2 = \\sum_{i=1}^{n}\\frac{(\\text{observerat}_i-\\text{modell}_i)^2}{\\text{modell}_i} \\] I människospråk: \\(\\chi^2\\) räknar kvadratskillnaden mellan det observerade cellvärdet och det förväntande cellvärdet, och dividerar detta med det förväntade cellvärdet. Sedan räknar den summan av alla dessa beräkningar för varje cellvärde. Slutresultatet är ett värde, \\(\\chi^2\\), som återspeglas i en \\(\\chi^2\\)-fördelning:\n\n\n\n\n\nHur räknas dessa skillnader? \\(\\chi^2\\) inleder med att räkna en modell för varje kombination av beroende och oberoende variablernas kategorier i korstabellen. Om vi t.ex. har två binära variabler, så summerar \\(\\chi^2\\) följande modeller: \\(\\text{modell}_{00}\\), \\(\\text{modell}_{01}\\), \\(\\text{modell}_{10}\\) och \\(\\text{modell}_{11}\\). Modellen räknas alltid ut genom att multiplicera radsumman för första variabeln med kolumnsumman för andra variabeln, och dividera resultatet med totala mängden observationer i datamaterialet \\(n\\).\nFör att exemplifiera detta kan vi ställa upp en kontingenstabell med påhittade siffror:\n\ndatamatris <- matrix(c(2, 7, 3, 5), nrow = 2)\ndatamatris\n\n     [,1] [,2]\n[1,]    2    3\n[2,]    7    5\n\n\nMed snabb huvudmatte kan vi räkna att vi har totalt \\(n = 2+3+7+5 = 17\\) observationer. Sedan räknar vi rad- och kolumnsummorna för varje kombination:\n\\[\\text{modell}_{11} = \\frac{(2+3) * (2+7)}{17} = 2.647, \\\\\n\\text{modell}_{12} = \\frac{(2+3) * (3+5)}{17} = 2.353, \\\\\n\\text{modell}_{21} = \\frac{(7+5) * (2+7)}{17} = 6.353, \\\\\n\\text{modell}_{22} = \\frac{(7+5) * (3+5)}{17} = 1.176\\]\nEfter detta slår vi in våra modellvärden i den slutliga formeln för \\(\\chi^2\\):\n\\[\\begin{equation}\n\\begin{split}\n  \\chi^2 & = \\sum_{i=1}^{n}\\frac{(\\text{observerat}_i-\\text{modell}_i)^2}{\\text{modell}_i} \\\\\n  & = \\frac{(2-2.647)^2}{2.647} + \\frac{(3-2.353)^2}{2.353} + \\frac{(7-6.353)^2}{6.353} + \\frac{(5-1.176)^2}{1.176} \\\\\n  & = 0.158 + 0.178 + 0.066 + 12.435 \\\\\n  & = 12.837\n\\end{split}\n\\end{equation}\\]\n\\(\\chi^2\\)-testet har en frihetsgrad enligt följande formel: \\(df = (r-1)(k-1)\\), alltså mängden av rader minus ett multiplicerat med mängden av kolumner minus ett. Du kan även tänka dig formeln som mängden av kategorier i beroende variabeln, minus ett, multiplicerat med mängden av kategorier i oberoende variabeln, minus ett.\nFör vår exempelberäkning är \\(df = (2-1)(2-1) = 1*1 = 1\\).\nDet slutliga värdet på \\(\\chi^2\\) varierar mellan 0-1. Testets storlek följer en \\(\\chi^2\\)-distribution (det var ju förvånande, det), så vi kan använda teststorleken för att få ett p-värde för nollhypotesen. De flesta \\(\\chi^2\\)-testfunktioner producerar detta p-värde färdigt, men om du inte har tillgång till p-värdet så kan du hämta ett värde med distributionsfunktionen stats::pchisq(q, df), där argumentet q är värdet och df är frihetsgraden.\nVår beräkning ovan ger följande resultat:\n\npchisq(12.837, df  = 2)\n\n[1] 0.9983689\n\n\nI R kan du utföra ett \\(\\chi^2\\)-test på två olika sätt. Om du har en färdig korstabell kan du kalla base::summary()-funktionen på den, vilket producerar testet:\n\nsummary(korstabell)\n\nCall: xtabs(formula = ~alcgp + tobgp, data = esoph)\nNumber of cases in table: 88 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 0.6195, df = 9, p-value = 0.9999\n    Chi-squared approximation may be incorrect\n\n\nDu kan även kalla funktionen stats::chisq.test() på tabellen, vilket utför samma sak. Med denna kan du dock få ytterligare information om testet, vilket kan hjälpa dig undersöka robustheten:\n\nchi.kvadrat = chisq.test(korstabell)\n\nWarning in chisq.test(korstabell): Chi-squared approximation may be incorrect\n\n\nIbland kan R kasta en varning i stilen av att “chi-kvadrat-approximeringen kan vara inkorrekt”. För summary()-funktionen står detta i output-texten. Detta innebär att en eller flera av de förväntande cellvärden underskrider 5, vilket brukar tas som en gräns för att lita på \\(\\chi^2\\)-testets robusthet. Du kan granska huruvida detta är sant genom att se på chisq.test(data)$expected (alla cellvärden borde vara större än 5):\n\nchi.kvadrat$expected\n\n           tobgp\nalcgp       0-9g/day    10-19    20-29      30+\n  0-39g/day 6.272727 6.272727 5.227273 5.227273\n  40-79     6.272727 6.272727 5.227273 5.227273\n  80-119    5.727273 5.727273 4.772727 4.772727\n  120+      5.727273 5.727273 4.772727 4.772727\n\n\n\n\nFörväntande värden under fem?\n\nOm du har problem med för låga förväntande värden, borde du i första hand samla mera data eller godkänna ditt öde. Det finns dock ett antal andra test som kan utföras på sådana datafördelningar.\n\nFishers exakta test fungerar på 2x2-tabeller, alltså korstabeller där bägge variabel är binär. Testet kan även räknas på större tabeller, men var förberedd på långa kalkylationstider. Använd funktionen stats::fisher.test(data).\nLikelihood ratio fungerar som \\(\\chi^2\\)-testet, men är robust för små urvalsstorlekar, alltså situationer där du kan finna förväntade cellfrekvenser under fem. R har ingen funktion för att kalkylera likelihood ratio för en korstabell. [LETA REDA PÅ EN!]\nCramérs V-test fungerar lite annorlunda än de andra testen, eftersom det ger en direkt effektstorlek. V varierar mellan 0-1 och är jämförbar med traditionella korrelationskoefficienter (mera om dem senare). Detta gör V-testet till ett mycket användbart test, och det är faktiskt att föredra att rapportera V om du vill jämföra effektstorleken med effekten i ett test som inte härstammar från en korstabell. Om du vill jämföra mellan korstabeller kan du alltid jämföra procentuella förändringar. Använd funktionen DescTools::CramerV(data). Du kan även hämta bootstrap-konfidensintervall för V med argumentet conf.level=x, där x är din konfidensnivå (t.ex. 0.95). Notera att detta kan ta en stund!\n\n\nFör visualisering av korstabeller kan vi förstås trycka själva tabellen, och detta är ett standardiserat sätt att uttrycka resultaten. Däremot kan vi även ställa upp resultaten grafiskt, med hjälp av en mosaikgraf. Mosaikgrafen uttrycker korstabellens frekvenser som en tvådimensionell “låda”. Du kan generera en mosaikgraf med funktionen graphics::mosaicplot(data):\n\nmosaicplot(datamatris)\n\n\n\n\nFrån grafen syns de relativa frekvenserna av varje kategori för rader och kolumner. Ta till exempel rad 1, kolumn 1 (cell 1.1): den tycks stå för ungefär en tredjedel av alla observationer på rad 1, och två femtedelar eller så av alla observationer i kolumn 1. Vi kan använda grafen för att få en snabb syn på huruvida vi tror att den oberoende variabeln (kolumnerna) är relaterad till den beroende variabeln (raderna). I vårt fall kunde det kanske ha varit så, eftersom cellstorlekarna är väldigt annorlunda (vilket innebär olika relativa frekvenser inom kategorierna), men vårt \\(\\chi^2\\)-test föreslog redan att detta trots allt inte var fallet.\nEftersom vårt \\(\\chi^2\\) uppvisade problem med de förväntade cellerna, kan vi använda Fishers exakta test för att få robusta resultat. Trots att testet är gjort för 2x2-tabeller går det att göra på större tabeller än så, och i vårt fall är analysen ännu någorlunda snabb:\n\nfisher.test(datamatris)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  datamatris\np-value = 0.6199\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.03039342 6.17614082\nsample estimates:\nodds ratio \n 0.4978506 \n\n\nP-värdet är 0.6199, vilket är högre än en rimlig alfatröskel (t.ex. \\(\\alpha = 0.05\\)), så vi kan inte förkasta nollhypotesen. Med andra ord: ett testvärde på detta (0.498) eller större är 62 % sannolikt om det riktiga testvärdet vore 13. Vår grafiska intution skulle ha fört oss fel!3 I detta fall testar vi odds ratio, vars “nolläge” är 1 istället för 0. För mera information, se kapitel XXX.\n\n\nFunktioner:\n\n\nxtabs(formula, data): Skapar en korstabell. Argumentet formula tar en formel på vad som ska korstabuleras, och data tar en mångdimensionell datastruktur. Formeln kan innehålla andra kontroller, se hjälpfunktionen ?xtabs för mer information.\npchisq(värde, df): Returnerar p-värdet för ett värde på \\(chi^2\\) med frihetsgraden df. Märk, att funktionen inte beaktar om kraven för \\(\\chi^2\\) har uppfyllts!\nchisq.test(data): Utför ett \\(\\chi^2\\)-test på data. Returnerar många olika statistiker, varav $expected är nyttig för att granska att alla förväntade cellvärden är över fem.\nfisher.test(data): Utför Fishers exakta test på data.\nCramerV(data): Räknar Cramérs V-test för data. För konfidensintervall, ge argumentet conf.level = x, där x är din konfidensnivå (t.ex. 0.95). Märk att beräkningen kan haka sig fast ibland, p.g.a. bootstrapping.\nmosaicplot(data): Ritar en mosaikgraf på data. Användbar för att snabbt granska huruvida korstabellen kunde vara värd att undersöka."
  },
  {
    "objectID": "bivariat-analys.html#students-t-test",
    "href": "bivariat-analys.html#students-t-test",
    "title": "12  Bivariat analys",
    "section": "12.2 Students T-test",
    "text": "12.2 Students T-test\nKorstabeller fungerar mycket bra för bivariata förhållanden mellan två kvalitativa variabler, men vad gör vi när ena variablen är kvantitativ? Svaret beror på vilken av våra variabler. Om vår beroende variabel är kvantitativ, men vår oberoende variabel är kvalitativ, så använder vi t-testet. Om situationen är tvärtemot, använder vi logistisk regression (vilket förklaras i samband med multivariata analyser).\nStudents t-test är skapat av statistikern William Sealy Gosset (1876-1937). Namnet kommer från att Gosset publicerade under pseudonymen “Student”, och termen “t-test” är egentligen lite fel: Gosset kallade testet för “t-statistiken”, vilket stod för “hypotesteststatistiken”. Vi fortsätter kalla det t-testet, eftersom namnet är välkänt, men märk att förkortningen då står för “hypotestesttestet”.\nT-testet mäter huruvida en skillnad kan anses vara tillräckligt stor för att den inte kunde orsakas under en nollhypotes - därför namnet “hypotestestet”. Vad denna skillnad är, och vilken nollhypotes som testas, beror på situationen. Vi kan tala om t-test för enkla urval och t-test för dubbla urval (eng. one- and two-sample t-tests).\nOavsett urvalsform följer t-testet en t-fördelning:\n\n\n\n\n\nSom du märker är t-fördelningen väldigt lik normalfördelningen. Den största skillnaden är att t-fördelningen har större svansar, alltså är den lite mera sensitiv för extrema värden.\nPå grund av att t-testet följer en fördelning, kan vi hämta sannolikhetsvärden för att en skillnad ger ett visst testvärde. För att göra detta bör vi även bestämma om vi testar två- eller ensidigt. Ett ensidigt t-test räknar ut sannolikheten att värdet är antingen större än, eller mindre än, det förväntade värdet. Du måste välja vilkendera du förväntar dig. Ett tvåsidigt t-test, å andra sidan, räknar ut sannolikheten att värdet skiljer sig från det förväntade värdet, oavsett vilken riktning. Dessa är något olika alternativa hypoteser, och påverkar tröskelvärdet för t-statistiken. Välj därför rätt!\n\n12.2.1 T-test för enkla urval\nOm vi testar skillnaden mellan ett urval och det förväntade populationsvärdet, talar vi om ett t-test för ett urval (eng. one-sample t-test). Då beräknas t-testet så här:\n\\[\\begin{equation}\n\\text{t} = \\frac{\\bar{x} - \\mu}{\\text{SD}/\\sqrt{n}}\n\\end{equation}\\]\noch i vanligt språk: t-statistiken är lika med det mätta medeltalet subtraherat med populationsmedeltalet, dividerat med standardavvikelsen dividerad med kvadratroten av urvalsstorleken.\nVi kan använda t-testet för enkla urval för att beräkna huruvida ett mätt medeltal är så pass mycket större (eller mindre) än något förväntat värde, att vi kan förkasta nollhypotesen att värdet är det förväntade värdet. Kom ihåg att välja om du vill testa för riktning (ensidigt) eller bara skillnad (tvåsidigt).\nEtt t-test för enkla urval kan utföras enkelt (ha!) i R med funktionen t.test():\n\nt.test(x,\n       y,\n       alternative = c(\"two-sided\", \"less\", \"greater\"),\n       mu = 0,\n       paired = FALSE,\n       var.equal = FALSE,\n       conf.level = 0.95,\n       ...)\n\nArgumentet x står för de data du vill testa. y är inte nödvändigt för t-test för enkla urval, men skulle användas för att testa skillnaden med andra data. alternative används för att bestämma hurudant test du gör: “two-sided” för tvåsidat test, “less” eller “greater” för ensidade test, antingen lägre än gränsvärdet eller högre än gränsvärdet. Med argumentet mu kan du ändra på vad nollhypotesen är; ofta testar du mot noll, men du kan byta det till vilken siffra som helst (enligt vad en logisk nollhypotes är för ditt syfte). paired används för att göra ett parat t-test - mer om detta senare. Argumentet var.equal bestämmer huruvida du gör ett test som är robust för variansheterogenitet eller inte. Till sist, conf.level bestämmer vilken konfidensnivå du vill använda. 0.95 motsvarar \\(\\alpha = 0.05\\), men du kan ändra konfidensnivån enligt behov.\nVi kan göra ett exempel. Säg att vi har mätt en massa data och vill veta om medeltalet på detta data skiljer sig statistiskt från 0, oavsett riktning. Eftersom vi inte är intresserade av riktning är det fråga om ett tvåsidigt test, och eftersom vi testar emot noll behöver vi inte ändra på mu-argumentet. Konfidensnivån håller vi på den vanliga 0.95-nivån, så vi behöver inte ändra på den heller.\n\ntestdata <- rnorm(100, mean = 2.5)\nt.test(testdata)\n\n\n    One Sample t-test\n\ndata:  testdata\nt = 22.175, df = 99, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 2.189935 2.620354\nsample estimates:\nmean of x \n 2.405145 \n\n\nMärk att jag inte behövde ge några som helst argument, förutom mitt data, åt t.test(). Detta är eftersom vi använde alla standardargument - vi gjorde tvåsidigt test mot nollhypotesen att värdet är 0, och vi bad om p-värde enligt konfidensnivån 0.95.\nTestresultatet berättar åt oss att det är frågan om ett T-test för enkelt urval (se rubriken). Vi får även veta våra storlekar: \\(\\text{t} = 27.131, \\text{df} = 99, \\text{p} = 2.2 * 10^{-16}\\). Testet berättar åt oss även det testade medeltalet, \\(\\hat{x} = 2.60\\), samt dess konfidensintervall på 0.95-nivån, \\(\\text{95% CI} = [2.41;2.80]\\). Därutöver säger den vad vi testade emot (“true mean is not equal to 0”), ifall vi lyckas glömma det.\n\n\nVarför finns det bokstäver i mitt p-värde?!\n\nNär vi jobbar med siffror hamnar vi ofta jobba med väldigt stora eller väldigt små siffror. Tänk dig att du ofta skulle jobba med siffror i miljardstorleken eller större (t.ex. statliga ekonomier eller hjärnceller). Det skulle bli väldigt svårt om din text är pepprad med siffror så som 1 000 000 000, 26 583 397 236 och 236 347 000.\nFör att underlätta läsningen använder vetenskapsidkare ofta vetenskaplig notation av siffror. Exempel siffrorna ovan kan skrivas på två sätt med vetenskaplig notation:\n\n\\(1.0 * 10^9, 26.5 * 10^9, 0.2*10^9\\)\n\\(1.0\\text{e}9, 26.5\\text{e}9, 0.2\\text{e}9\\)\n\nDetta förkortar siffrorna till en liten siffra, gånger en potens av tio. Du kan läsa dem som att potensstorleken (9 i exemplet) berättar åt dig hur många steg åt vänster du ska skuffa decimaltecknet. För att komma från 1.0 till en miljard ska du skuffa decimaltecknet 9 steg åt vänster, och lägga till nollor anefter. Bokstaven “e” i andra exemplet kan avläsas som “exponent”, alltså en förkortning för “tio upphöjt i…”\nEn annan vanlig situation där du bemöter vetenskaplig notation är med p-värden, eftersom de blir otroligt små. Samma sak gäller här, förutom att ett negativt tecken används för att påvisa att siffran är mindre än 1. Därmed är \\(0.05\\) samma sak som \\(5*10^{-2}\\), och \\(0.000000635\\) samma sak som \\(6.3 * 10^{-7}\\). Skuffa decimaltecknet åt höger istället för åt vänster.\nDet kan vara nyttigt att lära dig några vanliga tröskelvärden i vetenskaplig notation:\n\n\\(p = 0.05 = 5*10^{-2}\\)\n\\(p = 0.01 = 1*10^{-2}\\)\n\\(p = 0.001 = 1*10^{-3}\\)\n\n\n\n\n12.2.2 T-test för dubbla urval\nVi kan även använda t-testet för att testa skillnaden mellan två grupper i vårt urval. Detta kallas för ett t-test för dubbla urval, men trots namnet behöver du inte ha två separata urval. Vad testet gör är att det egentligen testar om de två grupperna härstammar från samma population. Detta gör att vi kan testa avsaknaden av olikhet4 mellan dessa två grupper.4 Se nedan för likhetstestning för att förstå varför avsaknaden av olikhet inte är detsamma som likhet.\nT-testet för dubbla urval ser ut på detta vis:\n\\[\\begin{equation}\n\\text{t} = \\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{\\frac{\\text{s}^2_1}{\\text{n}_1}+\\frac{\\text{s}^2_2}{\\text{n}_2}}}\n\\end{equation}\\]\nI klarspråk: Börja med att subtrahera medeltal 2 från medeltal 1. Sedan dividerar du det med följande: kvadratroten av, summan av varians 1 dividerat med urvalsstorlek 1 och varians 2 dividerat med urvalsstorlek 2.\nDetta test är faktiskt en mer robust form av dubbla urvalstestet, som kallas Welch t-test. Testet uppfanns av Bernard Lewis Welch, och modifierar det ursprungliga t-testet något. Till skillnad från de andra testen är denna variant av t-testet robust för olika gruppstorlekar och olika stora varianser. Det finns en orsak varför du inte skulle vilja använda Welch T-test: det är inte reliabelt i små urvalsstorlekar. Men, samma gäller för nästan alla bivariata test, så jag skulle inte ta huvudbry av det. Om du har för litet urval, samla mera data.\nDet dubbla t-testet utför du med samma funktion som tidigare, t.test()! Det är faktiskt så, att R utför Welch version av testet automatiskt - väldigt behändigt. Skillnaden till tidigare är att du nu ska ge testet två variabler (eftersom du ju testar skillnaden mellan två variabler), vilka ges med argumenten x och y:\n\na <- testdata\nb <- runif(100)\n\nt.test(x = a, y = b)\n\n\n    Welch Two Sample t-test\n\ndata:  a and b\nt = 17.275, df = 114.3, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1.722034 2.168135\nsample estimates:\nmean of x mean of y \n2.4051447 0.4600603 \n\n\nResultatet ser väldigt lika ut som för det enkla urvalets t-test, förutom att rubriken nu säger “Welch Two Sample t-test”. Testet berättar även åt oss vad vår alternativa hypotes denna gång är, samt att vi använde objekten a och b för vår analys. Vårt p-värde är väldigt lågt, och tydligt under en ordinarie gräns på \\(5*10^{-2}\\). Vi kan därmed med god sannolikhet förkasta vår nollhypotes: det vore väldigt sällsynt att få dessa resultat eller större om det faktiskt vore så, att bägge medeltal är desamma (alltså att skillnaden mellan dem är noll).\n\n\n12.2.3 Rapportering av t-test\nEtt t-test har tre viktiga kvantitativa mått: t-statistiken, frihetsgraderna och p-värdet. Alla tre bör rapporteras när du har gjort ett t-test. Utöver dessa kan konfidensintervallet vara bra om det finns en chans att resultatet upprepas och används i meta-analyser. Om du har gjort ändringar åt mu-koefficienten (gränsvärdet som testas emot) ska du rapportera det, annars förmodar läsare att du har testat emot noll:\nSkillnaden mellan X och Y på vår oberoende variabel är inte signifikant (t = 0.5, df = 12, p = 0.687).\nWelch robusta t-test visade på en signifikant skillnad mellan variablerna X och Y (t = 20.9, df = 117.28, p = 2 x 10^16, 95% CI av skillnaden [1.9; 2.3].\nDet finns inte några etablerade sätt att ställa upp ett t-test grafiskt, eftersom det enbart handlar om ett osäkerhetsmått (och dessa inte är lika intressanta som effektmåtten). Det är oftast bättre att rapportera t-testet i text, eftersom det är ett enkelt text med få delar att förstå. Du kan förvisso skapa lådgrafer av variablerna du jämförde, för att göra skillnaden visuellt tydlig. Kom dock ihåg att ställa in samma skala åt graferna, så att de faktiskt är jämförbara:\n\ndf <- tibble(a, b)\np1 <- ggplot() + geom_boxplot(aes(y = df$a)) + coord_cartesian(ylim=c(0,5))\np2 <- ggplot() + geom_boxplot(aes(y = df$b)) + coord_cartesian(ylim=c(0,5))\np1 | p2"
  },
  {
    "objectID": "bivariat-analys.html#likhetstest-och-minimistorlekstest",
    "href": "bivariat-analys.html#likhetstest-och-minimistorlekstest",
    "title": "12  Bivariat analys",
    "section": "12.3 Likhetstest och minimistorlekstest",
    "text": "12.3 Likhetstest och minimistorlekstest\nEtt traditionellt vetenskapligt ordspråk lyder: Avsaknaden av bevis är inte bevis för avsaknad. Med andra ord: bara för att du inte finner en endaste svart svan, betyder det inte att de inte finns.\nDetta medför ett litet problem till nollhypotessignifikanstestning: förkastande av nollhypotesen är inte bevis för alternativa hypotesen. Då du utför t.ex. ett t-test eller chi-kvadrat, och får ett signifikant värde, betyder det endast att det resultat du fått vore osannolikt (på din alfanivå, t.ex. 0.05) om det faktiskt vore så, att den verkliga skillnaden var noll. Detta säger inget om huruvida ditt resultat är sant.\nMer allmänt har detta varit ett problem för NHST: vetenskapsidkare vill ofta säga något om deras bevis, men NHST säger bara något om hypotesen. Lösningen, som tidigare sagt, kan vara Bayesisk statistik - istället för att undersöka hypotesens sannolikhet under data, kan man undersöka datas sannolikhet under hypotesen.\nEtt annat problem är vad som menas med “avsaknad”. På grund av mätfel har alla statistiska urval någon nivå av variation, vilket innebär att alla skillnader kommer att vara något ovan noll. Det kan handla om mycket små skillnader från noll, men trots allt skillnader från noll. Detta leder ofta till Typ I och Typ II fel (se kapitel @ref(statistisk-inferens)). Detta är ett problem både om vi vill testa ifall skillnaden är noll och om vi testar ifall skillnaden är annat än noll.\nEtt nyutvecklat verktyg, som behåller NHST-filosofin men möjliggör påståenden om avsaknad, är likhets- och minimistorlekstestning. Med dessa kan vi beakta den naturliga variationen av våra skillnadstest, och ändå säga något om den relevanta skillnaden vi undersöker. Dessa test utgörs av två olika, men relaterade test.\nEtt likhetstest (eng. equivalence test) testar huruvida två mätta medeltal är lika stora inom en viss likhetsgräns. Det är baserat på TOST-proceduren (eng. two one-sided T-tests), och görs genom att testa två olika hypoteser:\n\nNollhypotes 1: Skillnaden mellan medeltalen är större än A.\nNollhypotes 2: Skillnaden mellan medeltalen är mindre än B.\n\nVad vi gör i ett likhetstest, är att vi bestämmer ett intervall runt noll som innebär att skillnaden praktiskt taget är lika. Vad ska detta intervall vara? Det beror på vad vi mäter! TOST-proceduren inskriver en grad av subjektivitet i likhetstestning, men detta är inte dåligt. All statistik innehåller subjektiva omdömen, men i detta test gör vi det explicit.\nEfter att vi beräknat ensidiga T-test för dessa två hypoteser granskar vi deras signifikans (på den nivå vi förbestämt). Om bägge är signifikanta, kan vi säga att sannolikheten att få så pass likadana resultat som vi har fått är mycket låg om det vore så, att skillnaden i verkligheten är större/mindre än våra gränser.\nEtt minimieffekttest (eng. minimal effect test) är mycket likadant, men nu testar vi åt andra hållet:\n\nNollhypotes 1: Skillnaden mellan medeltalen är mindre än A.\nNollhypotes 2: Skillnaden mellan medeltalen är större än B.\n\nMed andra ord: nollhypoteserna påstår tillsammans att skillnaden ligger inom våra gränsen, inte utanför. Vi använder minimieffekttestet, så som namnet föreslår, för att testa huruvida skillnaden är praktiskt taget relevant, enligt de gränser vi lägger.\nOm du är klipsk, så märkte du säkert att minimieffekttestet är bara motsatsen av likhetstestet. Så är det! Orsaken till att vi behöver testet är aforismen jag nämnde tidigare: avsaknaden av bevis är inte bevis för avsaknad. Om ett likhetstest är icke-signifikant kan vi inte dra slutsatsen att skillnaden vi mätte är olika från noll, endast att det är sannolikt att få en sådan skillnad även då en sådan skillnad inte faktiskt finns. Detta kan te sig svårt att förstå, så kom åtminstone ihåg följande: avsaknaden av bevis är inte bevis för avsaknad. Ett hypotestest berättar endast om datas sannolikhet under nollhypotesen. Om du vill säga något om en alternativ hypotes, måste du omforma den till en ny nollhypotes.\nLikhets- och minimieffekttest kan göras enkelt med paketet TOSTER. I paketet finns funktionen t_TOST(), vilken tar nästan alla samma argument som t.test. Två nya argument är dock viktiga:\n\neqb: Storleken på likhetsgränsen, i en siffra. Detta skapar ett symmetriskt intervall runt jämförelsevärdet. Om du t.ex. testar likhet (skillnad = 0) med eqb = 0.5, så skapar den ett intervall på [-0.5, 0.5].\nhypothesis: Antingen “EQU” för likhet (equivalence) eller “MET” för minimieffekttest.\n\nDu kan även beakta olika varianser, ändra alfanivå, ändra mu, samt göra andra, mer avancerade ändringar till testet. Som exempel vill jag göra ett minimiskillnadstest på de data vi tidigare använde, med likhetsintervallet [-0.5, 0.5]:\n\nt_TOST(x = df$a,\n       y = df$b,\n       hypothesis=\"MET\",\n       var.equal=FALSE,\n       eqb = 0.5)\n\n\nWelch Two Sample t-test\n\nThe minimal effect test was significant, t(114.3) = 21.715, p = 3.41e-24\nThe null hypothesis test was significant, t(114.3) = 17.275, p = 1.19e-33\nNHST: reject null significance hypothesis that the effect is equal to zero \nTOST: reject null MET hypothesis\n\nTOST Results \n               t    df p.value\nt-test     17.27 114.3 < 0.001\nTOST Lower 21.72 114.3       1\nTOST Upper 12.83 114.3 < 0.001\n\nEffect Sizes \n               Estimate     SE             C.I. Conf. Level\nRaw               1.945 0.1126 [1.7584, 2.1318]         0.9\nHedges's g(av)    2.427 0.4500  [2.0727, 2.774]         0.9\nNote: SMD confidence intervals are an approximation. See vignette(\"SMD_calcs\").\n\n\nResultatet är tredelat: Först skriver testet ut en massa information i konsolen. Sedan ger den en dataram med T-statistikerna (resultat$TOST), och sedan en dataram med estimerade effektstorlekar (resultat$effsize). När vi läser av resultatet räcker det med konsolutskriften, men om du vill spara resultaten kan du senare använda t.ex. effektstorlekarna i grafiska uppsättningar.\nAtt tolka en TOST-procedur är lite mer komplext än ett vanligt t-test. Det första att komma ihåg är att det är frågan om två ensidiga t-test. [HUR TOLKAS DETTA?]"
  },
  {
    "objectID": "bivariat-analys.html#korrelationsanalys",
    "href": "bivariat-analys.html#korrelationsanalys",
    "title": "12  Bivariat analys",
    "section": "12.4 Korrelationsanalys",
    "text": "12.4 Korrelationsanalys"
  },
  {
    "objectID": "datamanipulering.html#faktorer",
    "href": "datamanipulering.html#faktorer",
    "title": "10  Datamanipulering",
    "section": "10.1 Faktorer",
    "text": "10.1 Faktorer\nI kapitel XXX behandlade jag statistiska variablers skalenivåer. Detta är ett så grundläggande koncept att R har inbyggda metoder angående skalenivåer. Datatypen factor, alltså faktor, är en sådan.\nFaktordata är data på nominal- eller ordinalskalenivå. Istället för att representera det i siffror kan R automatiskt förstå en variabel med några unika värden som en kvalitativ variabel. Tänk dig att du hade följande data:\n\npersonregister <- tibble(\"Person\" = c(\"Jimmy\",\"Claus\",\"Susan\",\"Freja\",\"Timotej\"),\n                         \"Kön\" = c(\"Man\",\"Man\",\"Kvinna\",\"Annat\",\"Man\"),\n                         \"Inkomst\" = c(25000,0,35000,30000,15000)\n                         )\n\n\n\n\n\nTable 10.1: Personregister\n\n\nPerson\nKön\nInkomst\n\n\n\n\nJimmy\nMan\n25000\n\n\nClaus\nMan\n0\n\n\nSusan\nKvinna\n35000\n\n\nFreja\nAnnat\n30000\n\n\nTimotej\nMan\n15000\n\n\n\n\n\n\nVariabeln Person fungerar i detta fall som en ID-variabel (eftersom varje namn är unikt), men kan också förstås som en nominalvariabel där de allra flesta värden kommer att vara unika. Förvisso kan det finnas flera personer med namnet “Jimmy”, kanske även “Timotej”, men för det mesta är dessa värden unika. Detta innebär att vi inte troligen vill göra mycket något med denna variabel. Dessutom representeras ju varje rad av en unik person, så summering enligt Person är inte så rimligt - vad betyder det att “Timotejer” har i medeltal 15 000 euro inkomst?\nVariabeln Kön representerar däremot en klassisk nominalvariabel. Det finns bara tre värden i vårt dataset, och vissa upprepas. Denna vill vi omvandla till en faktor, så att R vet att hantera variabeln som kategorisk!\nVariabeln Inkomst är tydligen på kvotskalenivå, eftersom den tycks representera euro (i året, om dessa personer inte är väldigt rika eller väldigt fattiga på en nordisk nivå). Då vill vi inte göra något åt denna variabel - den går att behandla som sådan i R-analyser.\nHur berättar vi åt R att Kön ska vara kategorisk? Med funktionen factor(data). Funktionen tar de data vi ger den, räknar unika värden, och markerar dessa som kategorier inom variabeln. Detta omvandlar variabeln till en speciell faktorvariabel, som går att använda i kvantitativa analyser (så länge kategoriska data är OK i modellen!). Vi utför detta i R:\n\npersonregister$Kön <- factor(personregister$Kön)\npersonregister\n\n# A tibble: 5 × 3\n  Person  Kön    Inkomst\n  <chr>   <fct>    <dbl>\n1 Jimmy   Man      25000\n2 Claus   Man          0\n3 Susan   Kvinna   35000\n4 Freja   Annat    30000\n5 Timotej Man      15000\n\n\nInga förändringar skedde till själva datapunkterna, och detta är vad som borde ske. Däremot har datatypen för Kön ändrat från character till factor - och detta möjliggör att den används i kategoriska analyser! Du kan se detta genom att granska kolumnnamnen i tibbleobjektet ovan.\nKan factor(data) beakta nivåernas ordning, om det är fråga om ordinalskalenivå? Jovisst! Låt oss säga att vi har en ny variabel, som mäter ångest:\n\n\n\n\n\n\n\nTable 10.2: Personregister med nya data\n\n\nPerson\nKön\nInkomst\nÅngest\n\n\n\n\nJimmy\nMan\n25000\nLåg\n\n\nClaus\nMan\n0\nHög\n\n\nSusan\nKvinna\n35000\nHög\n\n\nFreja\nAnnat\n30000\nMedel\n\n\nTimotej\nMan\n15000\nLåg\n\n\n\n\n\n\nFör att skapa en ordinalfaktor använder vi funktionen ordered(data, levels=x). Vi ger funktionen vårt data, personregister$Ångest, samt en vektor med alla variabelnivåer i ordning från högsta/största till lägsta/minsta. Egentligen kan vi även använda factor()-funktionen med tilläggsargumentet ordered = TRUE, men sättet jag gjorde det här är i min åsikt tydligare. Du gör förstås hur du vill.\nSom tidigare nämnt producerar detta ingen tydlig visuell skillnad - de facto var ångestvariabeln i Table 10.2 redan ordnad före jag berättade om ordningsfunktionen! Däremot har det en påverkan i vissa metoder, om de använder nominal- eller ordinalskalevariabler. Det är därför bra kutym att spara kvalitativa variabler som sådana, inte kvantitativa, även om det till synes inte gör något åt materialet.\n\n10.1.1 Omvandla sifferkodade faktorer till textkodade faktorer\nOm du kommer från ett annat statistikprogram har du kanske blivit van att spara kvalitativa variabler i sifferformat, med explicita kodböcker som berättar vilken siffra står för vilken kategori. Du skulle i såfall kanske ha velat koda Kön med siffrorna 1, 2 och 3, och sedan associerat siffrorna med etiketterna “Man”, “Kvinna” och “Annat”.\nI R är det bättre att använda de inbyggda faktor- och ordinalfaktordatatyperna, eftersom de förhindrar att du utför opasslig analys på datamaterialet. Om du sparar faktorer som siffror kan du i misstag analysera dem t.ex. med vanliga regressionsmetoder, även om detta bryter mot regressionsanalysens grundläggande krav (variablerna måste vara kvantitativa för regression).\nOm du dessutom har färdiga siffror i ditt datamaterial istället för textfaktorer kan det vara lockande att hålla kvar siffrorna. Du kan dock väldigt lätt omvandla kategorier med funktionen case_match() från paketet dplyr.11 En tidigare funktion som vissa läsare kan vara bekanta med, recode() ska vid skrivande stund avskrivas, så jag rekommenderar inte användningen av den.\nAtt omvandla från faktor till siffror (om du verkligen vill det) är väldigt lätt: case_match(datakolumn, kategori_1 ~ siffra, kategori_2 ~ siffra, ...). Lista alltså bara upp varje faktorkategori och ge den en siffra.\nOmvandlande från siffror till faktor är lite svårare. För det måste vi först köra case_match(datakolumn, siffra_1 ~ kategori, siffra_2 ~ kategori, ...) och sedan omvandla den resulterande textvektorn till en faktor:\n\n# Omvandla texten i \"Kön\" till siffror:\npersonregister$Kön <- case_match(personregister$Kön, \"Man\" ~ 1, \"Kvinna\" ~ 2, \"Annat\" ~ 3)\n\n# Omvandla siffrorna i \"Kön\" tillbaka till text:\n# OBS: Märk att siffrorna bör vara omringade av citattecken!\npersonregister$Kön <- case_match(personregister$Kön, 1 ~ \"Man\", 2 ~ \"Kvinna\", 3 ~ \"Annat\") |> factor(levels = c(\"Annat\", \"Kvinna\", \"Man\"))"
  },
  {
    "objectID": "datamanipulering.html#breda-och-långa-tabeller",
    "href": "datamanipulering.html#breda-och-långa-tabeller",
    "title": "10  Datamanipulering",
    "section": "10.2 Breda och långa tabeller",
    "text": "10.2 Breda och långa tabeller\nEtt annat viktigt koncept i"
  }
]